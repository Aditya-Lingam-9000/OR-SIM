{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d5dc15b",
   "metadata": {},
   "source": [
    "# Phase 3 — MedGemma Inference Test (Kaggle GPU)\n",
    "\n",
    "**Environment**: Kaggle, GPU T4×2 or P100 (≥15 GB VRAM)  \n",
    "**Model**: `medgemma-4b-it-Q3_K_M.gguf` (~2.1 GB) via `llama-cpp-python`  \n",
    "**Goal**: Validate that MedGemma correctly maps surgical transcriptions → structured JSON machine states\n",
    "\n",
    "Steps:\n",
    "1. Clone OR-SIM repo from GitHub\n",
    "2. Install llama-cpp-python with CUDA\n",
    "3. Copy GGUF model from Kaggle dataset\n",
    "4. Run 10 transcription samples per surgery (30 total)\n",
    "5. Measure inference latency\n",
    "6. Validate JSON output correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfa2f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 0. Check GPU ──────────────────────────────────────────────────────────\n",
    "import subprocess\n",
    "result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total', '--format=csv,noheader'],\n",
    "                        capture_output=True, text=True)\n",
    "print('GPU:', result.stdout.strip() or 'No GPU found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0dc2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 1. Clone OR-SIM repository ────────────────────────────────────────────\n",
    "# Replace <YOUR_GITHUB_USERNAME> with your actual username\n",
    "!git clone https://github.com/<YOUR_GITHUB_USERNAME>/OR-SIM.git /kaggle/working/OR-SIM\n",
    "%cd /kaggle/working/OR-SIM\n",
    "!git checkout dev\n",
    "!git log --oneline -5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d9f5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 2. Install llama-cpp-python with CUDA support ─────────────────────────\n",
    "# This builds from source with cuBLAS — takes ~3-5 minutes on Kaggle\n",
    "import os\n",
    "os.environ['CMAKE_ARGS'] = '-DLLAMA_CUBLAS=on'\n",
    "os.environ['FORCE_CMAKE'] = '1'\n",
    "\n",
    "!pip install llama-cpp-python --no-cache-dir -q\n",
    "\n",
    "# Verify GPU backend loaded\n",
    "from llama_cpp import Llama\n",
    "print('llama-cpp-python imported successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3273216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 3. Install OR-SIM Python dependencies ────────────────────────────────\n",
    "!pip install -r requirements/base.txt -q\n",
    "!pip install -r requirements/llm.txt -q\n",
    "print('Dependencies installed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd258dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 4. Locate GGUF model ──────────────────────────────────────────────────\n",
    "# Option A: Model is in a Kaggle dataset attached to this notebook\n",
    "# Option B: Upload the GGUF file manually to /kaggle/input/\n",
    "#\n",
    "# Update MODEL_PATH to point to your GGUF file:\n",
    "from pathlib import Path\n",
    "\n",
    "MODEL_PATH = Path('/kaggle/input/medgemma/medgemma-4b-it-Q3_K_M.gguf')\n",
    "\n",
    "# Fallback: search for any .gguf file\n",
    "if not MODEL_PATH.exists():\n",
    "    candidates = list(Path('/kaggle/input').rglob('*.gguf'))\n",
    "    if candidates:\n",
    "        MODEL_PATH = candidates[0]\n",
    "        print(f'Found GGUF at: {MODEL_PATH}')\n",
    "    else:\n",
    "        raise FileNotFoundError(\n",
    "            'No GGUF file found. Attach medgemma dataset to this notebook.'\n",
    "        )\n",
    "\n",
    "print(f'Model: {MODEL_PATH}  ({MODEL_PATH.stat().st_size / 1e9:.1f} GB)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ab9b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 5. Add OR-SIM to Python path ─────────────────────────────────────────\n",
    "import sys\n",
    "sys.path.insert(0, '/kaggle/working/OR-SIM')\n",
    "\n",
    "from backend.data.surgeries import SurgeryType, MACHINES\n",
    "from backend.llm.prompt_builder import PromptBuilder\n",
    "from backend.llm.output_parser  import parse_llm_output\n",
    "from backend.llm.schemas        import LLMOutput\n",
    "print('OR-SIM modules imported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586149e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 6. Load MedGemma model ────────────────────────────────────────────────\n",
    "import time\n",
    "\n",
    "t0 = time.time()\n",
    "llm = Llama(\n",
    "    model_path   = str(MODEL_PATH),\n",
    "    n_gpu_layers = -1,       # All layers on GPU\n",
    "    n_ctx        = 4096,\n",
    "    n_threads    = 4,\n",
    "    verbose      = False,\n",
    ")\n",
    "print(f'Model loaded in {time.time()-t0:.1f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2abce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 7. Test transcriptions per surgery ───────────────────────────────────\n",
    "TEST_CASES = {\n",
    "    SurgeryType.HEART_TRANSPLANT: [\n",
    "        'turn on the ventilator',\n",
    "        'activate bypass pump',\n",
    "        'turn off the OR lights',\n",
    "        'we need the defibrillator ready',\n",
    "        'activate echocardiography',\n",
    "        'turn off anesthesia machine',\n",
    "        'turn on cell saver',\n",
    "        'activate cardiac monitor',\n",
    "        'turn everything off',\n",
    "        'ventilator on, bypass pump on, OR lights on',\n",
    "    ],\n",
    "    SurgeryType.LIVER_RESECTION: [\n",
    "        'turn on the laparoscopic tower',\n",
    "        'activate the ultrasonic dissector',\n",
    "        'turn off the harmonic scalpel',\n",
    "        'we need the argon beam',\n",
    "        'activate the fluoroscopy unit',\n",
    "        'turn on the cell saver',\n",
    "        'activate the bipolar electrosurgery unit',\n",
    "        'turn off the OR lights',\n",
    "        'activate patient warmer',\n",
    "        'ultrasonic dissector and laparoscopic tower on',\n",
    "    ],\n",
    "    SurgeryType.KIDNEY_PCNL: [\n",
    "        'turn on the fluoroscopy C-arm',\n",
    "        'activate the nephroscope tower',\n",
    "        'turn on the lithotripter',\n",
    "        'we need the irrigation pump',\n",
    "        'activate the ultrasound guidance',\n",
    "        'turn off the electrosurgery unit',\n",
    "        'turn on the suction unit',\n",
    "        'activate OR lights',\n",
    "        'everything off please',\n",
    "        'fluoroscopy on, lithotripter on, nephroscope on',\n",
    "    ],\n",
    "}\n",
    "\n",
    "print('Test cases defined:', sum(len(v) for v in TEST_CASES.values()), 'total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4759231e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 8. Run inference on all test cases ───────────────────────────────────\n",
    "import json\n",
    "\n",
    "results = []\n",
    "\n",
    "for surgery, transcriptions in TEST_CASES.items():\n",
    "    builder = PromptBuilder(surgery)\n",
    "    print(f'\\n{'='*60}')\n",
    "    print(f'Surgery: {surgery.value}')\n",
    "    print('='*60)\n",
    "\n",
    "    for transcription in transcriptions:\n",
    "        messages = builder.build_messages(transcription, snapshot=None)\n",
    "\n",
    "        t0 = time.time()\n",
    "        response = llm.create_chat_completion(\n",
    "            messages    = messages,\n",
    "            max_tokens  = 256,\n",
    "            temperature = 0.1,\n",
    "            top_p       = 0.9,\n",
    "        )\n",
    "        latency_ms = (time.time() - t0) * 1000\n",
    "\n",
    "        raw_text = response['choices'][0]['message']['content']\n",
    "        parsed   = parse_llm_output(raw_text, surgery)\n",
    "\n",
    "        results.append({\n",
    "            'surgery':      surgery.value,\n",
    "            'transcription': transcription,\n",
    "            'raw_output':   raw_text,\n",
    "            'parsed_on':    parsed.machine_states['1'],\n",
    "            'parsed_off':   parsed.machine_states['0'],\n",
    "            'reasoning':    parsed.reasoning,\n",
    "            'latency_ms':   latency_ms,\n",
    "        })\n",
    "\n",
    "        print(f'\\nTranscription : {transcription}')\n",
    "        print(f'ON            : {parsed.machine_states[\"1\"]}')\n",
    "        print(f'OFF           : {parsed.machine_states[\"0\"]}')\n",
    "        print(f'Reasoning     : {parsed.reasoning[:80]}...' if len(parsed.reasoning) > 80 else f'Reasoning     : {parsed.reasoning}')\n",
    "        print(f'Latency       : {latency_ms:.0f} ms')\n",
    "\n",
    "print(f'\\nTotal inference calls: {len(results)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca0b9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 9. Latency summary ───────────────────────────────────────────────────\n",
    "import statistics\n",
    "\n",
    "latencies = [r['latency_ms'] for r in results]\n",
    "print('Latency Statistics (ms):')\n",
    "print(f'  Min    : {min(latencies):.0f}')\n",
    "print(f'  Max    : {max(latencies):.0f}')\n",
    "print(f'  Mean   : {statistics.mean(latencies):.0f}')\n",
    "print(f'  Median : {statistics.median(latencies):.0f}')\n",
    "print(f'  P95    : {sorted(latencies)[int(len(latencies)*0.95)]:.0f}')\n",
    "\n",
    "target_ms = 3000\n",
    "under_target = sum(1 for l in latencies if l < target_ms)\n",
    "print(f'\\nUnder {target_ms}ms target: {under_target}/{len(latencies)} ({100*under_target/len(latencies):.0f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5395587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 10. Correctness check ────────────────────────────────────────────────\n",
    "# Expected: at least one machine correctly ON for each transcription\n",
    "# (Heuristic — not exact as MedGemma may interpret commands differently)\n",
    "\n",
    "keyword_to_machines = {\n",
    "    'ventilator':    ['Ventilator'],\n",
    "    'bypass pump':   ['Bypass Pump', 'Cardiopulmonary Bypass Machine'],\n",
    "    'defibrillator': ['Defibrillator'],\n",
    "    'OR lights':     ['OR Lights'],\n",
    "    'laparoscopic':  ['Laparoscopic Tower'],\n",
    "    'lithotripter':  ['Lithotripter'],\n",
    "    'flouroscop':    ['Fluoroscopy C-Arm'],\n",
    "    'fluoroscop':    ['Fluoroscopy C-Arm'],\n",
    "    'C-arm':         ['Fluoroscopy C-Arm'],\n",
    "}\n",
    "\n",
    "correct   = 0\n",
    "incorrect = 0\n",
    "\n",
    "for r in results:\n",
    "    tr = r['transcription'].lower()\n",
    "    on = [m.lower() for m in r['parsed_on']]\n",
    "\n",
    "    matched = False\n",
    "    for keyword, expected_machines in keyword_to_machines.items():\n",
    "        if keyword in tr:\n",
    "            for em in expected_machines:\n",
    "                if any(em.lower() in o for o in on):\n",
    "                    matched = True\n",
    "                    break\n",
    "    \n",
    "    # If no keyword match in our heuristic dict, skip the check\n",
    "    if any(kw in tr for kw in keyword_to_machines):\n",
    "        if matched:\n",
    "            correct += 1\n",
    "        else:\n",
    "            incorrect += 1\n",
    "            print(f'POTENTIAL MISS: \"{r[\"transcription\"]}\" → ON={r[\"parsed_on\"]}')\n",
    "\n",
    "checked = correct + incorrect\n",
    "if checked > 0:\n",
    "    print(f'\\nHeuristic correctness: {correct}/{checked} ({100*correct/checked:.0f}%)')\n",
    "else:\n",
    "    print('No keyword-matched heuristics to check')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779323e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 11. Save results to JSON ─────────────────────────────────────────────\n",
    "output_path = Path('/kaggle/working/phase3_results.json')\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "print(f'Results saved to {output_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0b99ce",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Expected results for Phase 3 sign-off:\n",
    "- ✅ MedGemma loads successfully with `n_gpu_layers=-1`\n",
    "- ✅ Median inference latency < 3000 ms on T4 GPU\n",
    "- ✅ JSON output parses correctly for all 30 test cases\n",
    "- ✅ Heuristic keyword correctness > 80%\n",
    "\n",
    "If all pass → Phase 3 complete → proceed to Phase 4 (E2E Pipeline)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
