{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d5dc15b",
   "metadata": {},
   "source": [
    "# Phase 3 — MedGemma Inference Test (Kaggle GPU)\n",
    "\n",
    "**Environment**: Kaggle, GPU T4×2 or P100 (≥15 GB VRAM)  \n",
    "**Model**: `medgemma-4b-it-Q3_K_M.gguf` (~2.1 GB) via `llama-cpp-python` (CUDA 12.4 prebuilt wheels — no compilation)  \n",
    "**Goal**: Validate that MedGemma correctly maps surgical transcriptions → structured JSON machine states\n",
    "\n",
    "Steps:\n",
    "1. Check GPU\n",
    "2. Clone OR-SIM repo (`main` branch)\n",
    "3. Install OR-SIM Python dependencies\n",
    "4. Install `llama-cpp-python` from prebuilt CUDA 12.4 wheels *(fast — ~30s, no compilation)*\n",
    "5. Download MedGemma GGUF from HuggingFace (`unsloth/medgemma-4b-it-GGUF`)\n",
    "6. Verify GPU offload is active\n",
    "7. Run 10 transcription samples per surgery (30 total)\n",
    "8. Measure latency & validate JSON output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfa2f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 0. Check GPU ──────────────────────────────────────────────────────────\n",
    "import subprocess\n",
    "result = subprocess.run(\n",
    "    ['nvidia-smi', '--query-gpu=name,memory.total,driver_version', '--format=csv,noheader'],\n",
    "    capture_output=True, text=True\n",
    ")\n",
    "print('GPU:', result.stdout.strip() or 'No GPU found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0dc2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 1. Clone OR-SIM repository (main branch) ─────────────────────────────\n",
    "# Replace <YOUR_GITHUB_USERNAME> with your actual GitHub username before running\n",
    "!git clone https://github.com/<YOUR_GITHUB_USERNAME>/OR-SIM.git /kaggle/working/OR-SIM\n",
    "%cd /kaggle/working/OR-SIM\n",
    "!git log --oneline -5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d9f5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 2. Install llama-cpp-python — CUDA 12.4 prebuilt wheels (no compilation)\n",
    "# Uses Kaggle's T4 CUDA 12.4 driver — takes ~30s vs ~5 minutes if building from source\n",
    "!pip install llama-cpp-python \\\n",
    "    --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu124\n",
    "\n",
    "# Verify GPU offload is compiled in — must be True\n",
    "import llama_cpp\n",
    "lib = getattr(llama_cpp, 'llama_cpp', None)\n",
    "gpu_ok = lib.llama_supports_gpu_offload() if lib else False\n",
    "print(f'llama_supports_gpu_offload() = {gpu_ok}')\n",
    "if not gpu_ok:\n",
    "    raise RuntimeError('GPU offload not available — check Kaggle CUDA version matches cu124 wheel')\n",
    "\n",
    "from llama_cpp import Llama\n",
    "print('llama-cpp-python imported successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3273216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 3. Install OR-SIM Python dependencies ────────────────────────────────\n",
    "!pip install -q -r /kaggle/working/OR-SIM/requirements/base.txt\n",
    "!pip install -q -r /kaggle/working/OR-SIM/requirements/llm.txt\n",
    "print('OR-SIM dependencies installed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd258dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 4. Download MedGemma GGUF from HuggingFace ───────────────────────────\n",
    "!pip install -q huggingface_hub\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "os.makedirs('/kaggle/working/OR-SIM/models/medgemma', exist_ok=True)\n",
    "\n",
    "MODEL_PATH = hf_hub_download(\n",
    "    repo_id   = 'unsloth/medgemma-4b-it-GGUF',\n",
    "    filename  = 'medgemma-4b-it-Q3_K_M.gguf',\n",
    "    local_dir = '/kaggle/working/OR-SIM/models/medgemma',\n",
    ")\n",
    "MODEL_PATH = Path(MODEL_PATH)\n",
    "print(f'Model downloaded to: {MODEL_PATH}')\n",
    "print(f'File size          : {MODEL_PATH.stat().st_size / 1e9:.2f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ab9b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 5. Add OR-SIM to Python path ─────────────────────────────────────────\n",
    "import sys\n",
    "sys.path.insert(0, '/kaggle/working/OR-SIM')\n",
    "\n",
    "from backend.data.surgeries import SurgeryType, MACHINES\n",
    "from backend.llm.prompt_builder import PromptBuilder\n",
    "from backend.llm.output_parser  import parse_llm_output\n",
    "from backend.llm.schemas        import LLMOutput\n",
    "print('OR-SIM modules imported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586149e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 6. Load MedGemma model ────────────────────────────────────────────────\n",
    "import time\n",
    "\n",
    "t0 = time.time()\n",
    "llm = Llama(\n",
    "    model_path   = str(MODEL_PATH),\n",
    "    n_gpu_layers = -1,       # All layers on GPU\n",
    "    n_ctx        = 4096,\n",
    "    n_threads    = 4,\n",
    "    verbose      = False,\n",
    ")\n",
    "print(f'Model loaded in {time.time()-t0:.1f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2abce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 7. Test transcriptions per surgery ───────────────────────────────────   # NOTE: each test case exercises machines that actually exist in that surgery.\n",
    "# Liver Resection has no fluoroscopy — removed that case.\n",
    "TEST_CASES = {\n",
    "    SurgeryType.HEART_TRANSPLANT: [\n",
    "        'turn on the ventilator',\n",
    "        'activate bypass pump',\n",
    "        'turn off the OR lights',\n",
    "        'we need the defibrillator ready',\n",
    "        'activate the cardiac monitor',\n",
    "        'turn off anesthesia machine',\n",
    "        'turn on the perfusion pump',\n",
    "        'activate the warming blanket',\n",
    "        'turn everything off',\n",
    "        'ventilator on, bypass pump on, OR lights on',\n",
    "    ],\n",
    "    SurgeryType.LIVER_RESECTION: [\n",
    "        'turn on the laparoscopic tower',\n",
    "        'activate the ultrasonic dissector',\n",
    "        'turn off the harmonic scalpel',\n",
    "        'we need the argon beam coagulator',\n",
    "        'activate the CO2 insufflator',\n",
    "        'turn on the cell saver',\n",
    "        'activate the bipolar electrosurgery unit',\n",
    "        'turn off the OR lights',\n",
    "        'activate patient warmer',\n",
    "        'ultrasonic dissector and laparoscopic tower on',\n",
    "    ],\n",
    "    SurgeryType.KIDNEY_PCNL: [\n",
    "        'turn on the fluoroscopy C-arm',\n",
    "        'activate the nephroscope',\n",
    "        'turn on the lithotripter',\n",
    "        'we need the irrigation pump',\n",
    "        'activate the ultrasound guidance',\n",
    "        'turn off the electrosurgery unit',\n",
    "        'turn on the suction unit',\n",
    "        'activate OR lights',\n",
    "        'everything off please',\n",
    "        'fluoroscopy on, lithotripter on, nephroscope on',\n",
    "    ],\n",
    "}\n",
    "\n",
    "print('Test cases defined:', sum(len(v) for v in TEST_CASES.values()), 'total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4759231e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 8. Run inference on all test cases ───────────────────────────────────\n",
    "import json\n",
    "\n",
    "results = []\n",
    "sep = '=' * 60\n",
    "\n",
    "for surgery, transcriptions in TEST_CASES.items():\n",
    "    builder = PromptBuilder(surgery)\n",
    "    print(f'\\n{sep}')\n",
    "    print(f'Surgery: {surgery.value}')\n",
    "    print(sep)\n",
    "\n",
    "    for transcription in transcriptions:\n",
    "        messages = builder.build_messages(transcription, snapshot=None)\n",
    "\n",
    "        t0 = time.time()\n",
    "        response = llm.create_chat_completion(\n",
    "            messages    = messages,\n",
    "            max_tokens  = 256,\n",
    "            temperature = 0.1,\n",
    "            top_p       = 0.9,\n",
    "        )\n",
    "        latency_ms = (time.time() - t0) * 1000\n",
    "\n",
    "        raw_text = response['choices'][0]['message']['content']\n",
    "        parsed   = parse_llm_output(raw_text, surgery)\n",
    "\n",
    "        results.append({\n",
    "            'surgery':       surgery.value,\n",
    "            'transcription': transcription,\n",
    "            'raw_output':    raw_text,\n",
    "            'parsed_on':     parsed.machine_states['1'],\n",
    "            'parsed_off':    parsed.machine_states['0'],\n",
    "            'reasoning':     parsed.reasoning,\n",
    "            'latency_ms':    latency_ms,\n",
    "        })\n",
    "\n",
    "        reasoning_preview = (parsed.reasoning[:80] + '...') if len(parsed.reasoning) > 80 else parsed.reasoning\n",
    "        print(f'\\nTranscription : {transcription}')\n",
    "        print(f'ON            : {parsed.machine_states[\"1\"]}')\n",
    "        print(f'OFF           : {parsed.machine_states[\"0\"]}')\n",
    "        print(f'Reasoning     : {reasoning_preview}')\n",
    "        print(f'Latency       : {latency_ms:.0f} ms')\n",
    "\n",
    "print(f'\\nTotal inference calls: {len(results)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca0b9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 9. Latency summary ───────────────────────────────────────────────────\n",
    "import statistics\n",
    "\n",
    "latencies = [r['latency_ms'] for r in results]\n",
    "print('Latency Statistics (ms):')\n",
    "print(f'  Min    : {min(latencies):.0f}')\n",
    "print(f'  Max    : {max(latencies):.0f}')\n",
    "print(f'  Mean   : {statistics.mean(latencies):.0f}')\n",
    "print(f'  Median : {statistics.median(latencies):.0f}')\n",
    "print(f'  P95    : {sorted(latencies)[int(len(latencies)*0.95)]:.0f}')\n",
    "\n",
    "target_ms = 3000\n",
    "under_target = sum(1 for lat in latencies if lat < target_ms)\n",
    "print(f'\\nUnder {target_ms}ms target: {under_target}/{len(latencies)} ({100*under_target/len(latencies):.0f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5395587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 10. Correctness check ────────────────────────────────────────────────\n",
    "# Dynamic heuristic: for each result, scan the transcription for any alias\n",
    "# or canonical name from that surgery's MACHINES dict.\n",
    "# If found → check the model put that canonical in the right list.\n",
    "# \"turn off X\" checks turn_off; otherwise checks turn_on.\n",
    "\n",
    "from difflib import get_close_matches\n",
    "\n",
    "def find_expected_machine(transcription: str, surgery: 'SurgeryType') -> 'tuple[str|None, str]':\n",
    "    \"\"\"\n",
    "    Search transcription for any machine alias/name from MACHINES.\n",
    "    Returns (canonical_name, direction) where direction is 'on' or 'off'.\n",
    "    Returns (None, 'on') if no machine keyword is found.\n",
    "    \"\"\"\n",
    "    tr = transcription.lower()\n",
    "    # Determine direction from command verb\n",
    "    # \"turn off X\", \"deactivate X\", \"shut down X\" → off\n",
    "    off_verbs = ('turn off', 'deactivate', 'shut down', 'stop', 'disable')\n",
    "    direction = 'off' if any(v in tr for v in off_verbs) else 'on'\n",
    "    # \"everything off\" / \"all off\" special case\n",
    "    if any(p in tr for p in ('everything off', 'all off', 'turn everything', 'all equipment off')):\n",
    "        return '__ALL__', 'off'\n",
    "\n",
    "    # Build alias→canonical map for this surgery\n",
    "    alias_map = {}\n",
    "    for machine in MACHINES[surgery].values():\n",
    "        canonical = machine['name']\n",
    "        alias_map[canonical.lower()] = canonical\n",
    "        for alias in machine.get('aliases', []):\n",
    "            alias_map[alias.lower()] = canonical\n",
    "\n",
    "    # 1. Exact/substring match against all aliases\n",
    "    best_match = None\n",
    "    best_len   = 0\n",
    "    for alias_key, canonical in alias_map.items():\n",
    "        if alias_key in tr and len(alias_key) > best_len:\n",
    "            best_match = canonical\n",
    "            best_len   = len(alias_key)\n",
    "    if best_match:\n",
    "        return best_match, direction\n",
    "\n",
    "    # 2. difflib on canonical names\n",
    "    canonical_names  = [m['name'] for m in MACHINES[surgery].values()]\n",
    "    lower_canonicals = [c.lower() for c in canonical_names]\n",
    "    words = tr.split()\n",
    "    for word in words:\n",
    "        if len(word) < 4:\n",
    "            continue\n",
    "        hits = get_close_matches(word, lower_canonicals, n=1, cutoff=0.72)\n",
    "        if hits:\n",
    "            for c in canonical_names:\n",
    "                if c.lower() == hits[0]:\n",
    "                    return c, direction\n",
    "\n",
    "    return None, direction\n",
    "\n",
    "\n",
    "correct = incorrect = skipped = 0\n",
    "sep = '-' * 60\n",
    "\n",
    "for r in results:\n",
    "    surgery_type = next(s for s in SurgeryType if s.value == r['surgery'])\n",
    "    expected_machine, direction = find_expected_machine(r['transcription'], surgery_type)\n",
    "\n",
    "    if expected_machine is None:\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "    if expected_machine == '__ALL__':\n",
    "        # \"turn everything off\" — expect at least one machine in turn_off\n",
    "        if r['parsed_off']:\n",
    "            correct += 1\n",
    "        else:\n",
    "            incorrect += 1\n",
    "            print(f'MISS [everything-off]: \"{r[\"transcription\"]}\" → OFF={r[\"parsed_off\"]}')\n",
    "        continue\n",
    "\n",
    "    target_list = r['parsed_on'] if direction == 'on' else r['parsed_off']\n",
    "    target_lower = [m.lower() for m in target_list]\n",
    "\n",
    "    # Accept exact match OR expected name is substring of model output\n",
    "    hit = any(\n",
    "        expected_machine.lower() == m or expected_machine.lower() in m\n",
    "        for m in target_lower\n",
    "    )\n",
    "\n",
    "    if hit:\n",
    "        correct += 1\n",
    "    else:\n",
    "        incorrect += 1\n",
    "        arrow = 'ON' if direction == 'on' else 'OFF'\n",
    "        actual = r['parsed_on'] if direction == 'on' else r['parsed_off']\n",
    "        print(f'MISS: \"{r[\"transcription\"]}\"')\n",
    "        print(f'      expected {arrow}: {expected_machine!r}')\n",
    "        print(f'      got     {arrow}: {actual}')\n",
    "\n",
    "checked = correct + incorrect\n",
    "print(f'\\n{sep}')\n",
    "if checked > 0:\n",
    "    pct = 100 * correct / checked\n",
    "    print(f'Heuristic correctness : {correct}/{checked} ({pct:.0f}%)')\n",
    "    verdict = '✅ PASS (≥80%)' if pct >= 80 else '❌ FAIL (<80%)'\n",
    "    print(f'Target ≥80%           : {verdict}')\n",
    "else:\n",
    "    print('No keyword-matched heuristics to evaluate')\n",
    "print(f'Skipped (no heuristic): {skipped}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779323e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 11. Save results to JSON ─────────────────────────────────────────────\n",
    "from pathlib import Path\n",
    "\n",
    "output_path = Path('/kaggle/working/phase3_results.json')\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "print(f'Results saved to {output_path}')\n",
    "print(f'Total calls  : {len(results)}')\n",
    "print(f'Avg latency  : {statistics.mean(latencies):.0f} ms')\n",
    "print(f'Median       : {statistics.median(latencies):.0f} ms')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0b99ce",
   "metadata": {},
   "source": [
    "## Phase 3 Sign-off Criteria\n",
    "\n",
    "| Check | Target | How verified |\n",
    "|-------|--------|-------------|\n",
    "| MedGemma loads without error | ✅ no exception | Cell 6 |\n",
    "| `llama_supports_gpu_offload()` returns `True` | ✅ must be True | Cell 2 |\n",
    "| All 30 JSON responses parse successfully | 30 / 30 non-empty `machine_states` | Cell 8 |\n",
    "| Median inference latency | < 3 000 ms | Cell 9 |\n",
    "| Heuristic keyword correctness | > 80 % | Cell 10 |\n",
    "\n",
    "If all pass → Phase 3 complete → report results → **proceed to Phase 4: E2E Pipeline**  \n",
    "(`LiveTranscriber → MedGemmaModel → StateManager → machine_states.json`)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
