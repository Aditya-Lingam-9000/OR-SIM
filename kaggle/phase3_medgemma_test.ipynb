{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d5dc15b",
   "metadata": {},
   "source": [
    "# Phase 3 — MedGemma Inference Test (Kaggle GPU)\n",
    "\n",
    "**Environment**: Kaggle, GPU T4×2 or P100 (≥15 GB VRAM)  \n",
    "**Model**: `medgemma-4b-it-Q3_K_M.gguf` (~2.1 GB) via `llama-cpp-python` (CUDA 12.4 prebuilt wheels — no compilation)  \n",
    "**Goal**: Validate that MedGemma correctly maps surgical transcriptions → structured JSON machine states\n",
    "\n",
    "Steps:\n",
    "1. Check GPU\n",
    "2. Clone OR-SIM repo (`main` branch)\n",
    "3. Install OR-SIM Python dependencies\n",
    "4. Install `llama-cpp-python` from prebuilt CUDA 12.4 wheels *(fast — ~30s, no compilation)*\n",
    "5. Download MedGemma GGUF from HuggingFace (`unsloth/medgemma-4b-it-GGUF`)\n",
    "6. Verify GPU offload is active\n",
    "7. Run 10 transcription samples per surgery (30 total)\n",
    "8. Measure latency & validate JSON output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfa2f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 0. Check GPU ──────────────────────────────────────────────────────────\n",
    "import subprocess\n",
    "result = subprocess.run(\n",
    "    ['nvidia-smi', '--query-gpu=name,memory.total,driver_version', '--format=csv,noheader'],\n",
    "    capture_output=True, text=True\n",
    ")\n",
    "print('GPU:', result.stdout.strip() or 'No GPU found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0dc2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 1. Clone OR-SIM repository (main branch) ─────────────────────────────\n",
    "# Replace <YOUR_GITHUB_USERNAME> with your actual GitHub username before running\n",
    "!git clone https://github.com/<YOUR_GITHUB_USERNAME>/OR-SIM.git /kaggle/working/OR-SIM\n",
    "%cd /kaggle/working/OR-SIM\n",
    "!git log --oneline -5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d9f5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 2. Install llama-cpp-python — CUDA 12.4 prebuilt wheels (no compilation)\n",
    "# Uses Kaggle's T4 CUDA 12.4 driver — takes ~30s vs ~5 minutes if building from source\n",
    "!pip install llama-cpp-python \\\n",
    "    --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu124\n",
    "\n",
    "# Verify GPU offload is compiled in — must be True\n",
    "import llama_cpp\n",
    "lib = getattr(llama_cpp, 'llama_cpp', None)\n",
    "gpu_ok = lib.llama_supports_gpu_offload() if lib else False\n",
    "print(f'llama_supports_gpu_offload() = {gpu_ok}')\n",
    "if not gpu_ok:\n",
    "    raise RuntimeError('GPU offload not available — check Kaggle CUDA version matches cu124 wheel')\n",
    "\n",
    "from llama_cpp import Llama\n",
    "print('llama-cpp-python imported successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3273216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 3. Install OR-SIM Python dependencies ────────────────────────────────\n",
    "!pip install -q -r /kaggle/working/OR-SIM/requirements/base.txt\n",
    "!pip install -q -r /kaggle/working/OR-SIM/requirements/llm.txt\n",
    "print('OR-SIM dependencies installed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd258dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 4. Download MedGemma GGUF from HuggingFace ───────────────────────────\n",
    "!pip install -q huggingface_hub\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "os.makedirs('/kaggle/working/OR-SIM/models/medgemma', exist_ok=True)\n",
    "\n",
    "MODEL_PATH = hf_hub_download(\n",
    "    repo_id   = 'unsloth/medgemma-4b-it-GGUF',\n",
    "    filename  = 'medgemma-4b-it-Q3_K_M.gguf',\n",
    "    local_dir = '/kaggle/working/OR-SIM/models/medgemma',\n",
    ")\n",
    "MODEL_PATH = Path(MODEL_PATH)\n",
    "print(f'Model downloaded to: {MODEL_PATH}')\n",
    "print(f'File size          : {MODEL_PATH.stat().st_size / 1e9:.2f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ab9b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 5. Add OR-SIM to Python path ─────────────────────────────────────────\n",
    "import sys\n",
    "sys.path.insert(0, '/kaggle/working/OR-SIM')\n",
    "\n",
    "from backend.data.surgeries import SurgeryType, MACHINES\n",
    "from backend.llm.prompt_builder import PromptBuilder\n",
    "from backend.llm.output_parser  import parse_llm_output\n",
    "from backend.llm.schemas        import LLMOutput\n",
    "print('OR-SIM modules imported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586149e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 6. Load MedGemma model ────────────────────────────────────────────────\n",
    "import time\n",
    "\n",
    "t0 = time.time()\n",
    "llm = Llama(\n",
    "    model_path   = str(MODEL_PATH),\n",
    "    n_gpu_layers = -1,       # All layers on GPU\n",
    "    n_ctx        = 4096,\n",
    "    n_threads    = 4,\n",
    "    verbose      = False,\n",
    ")\n",
    "print(f'Model loaded in {time.time()-t0:.1f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2abce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 7. Test transcriptions per surgery ───────────────────────────────────\n",
    "TEST_CASES = {\n",
    "    SurgeryType.HEART_TRANSPLANT: [\n",
    "        'turn on the ventilator',\n",
    "        'activate bypass pump',\n",
    "        'turn off the OR lights',\n",
    "        'we need the defibrillator ready',\n",
    "        'activate the cardiac monitor',\n",
    "        'turn off anesthesia machine',\n",
    "        'turn on the perfusion pump',\n",
    "        'activate the warming blanket',\n",
    "        'turn everything off',\n",
    "        'ventilator on, bypass pump on, OR lights on',\n",
    "    ],\n",
    "    SurgeryType.LIVER_RESECTION: [\n",
    "        'turn on the laparoscopic tower',\n",
    "        'activate the ultrasonic dissector',\n",
    "        'turn off the harmonic scalpel',\n",
    "        'we need the argon beam',\n",
    "        'activate the fluoroscopy unit',\n",
    "        'turn on the cell saver',\n",
    "        'activate the bipolar electrosurgery unit',\n",
    "        'turn off the OR lights',\n",
    "        'activate patient warmer',\n",
    "        'ultrasonic dissector and laparoscopic tower on',\n",
    "    ],\n",
    "    SurgeryType.KIDNEY_PCNL: [\n",
    "        'turn on the fluoroscopy C-arm',\n",
    "        'activate the nephroscope tower',\n",
    "        'turn on the lithotripter',\n",
    "        'we need the irrigation pump',\n",
    "        'activate the ultrasound guidance',\n",
    "        'turn off the electrosurgery unit',\n",
    "        'turn on the suction unit',\n",
    "        'activate OR lights',\n",
    "        'everything off please',\n",
    "        'fluoroscopy on, lithotripter on, nephroscope on',\n",
    "    ],\n",
    "}\n",
    "\n",
    "print('Test cases defined:', sum(len(v) for v in TEST_CASES.values()), 'total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4759231e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 8. Run inference on all test cases ───────────────────────────────────\n",
    "import json\n",
    "\n",
    "results = []\n",
    "sep = '=' * 60\n",
    "\n",
    "for surgery, transcriptions in TEST_CASES.items():\n",
    "    builder = PromptBuilder(surgery)\n",
    "    print(f'\\n{sep}')\n",
    "    print(f'Surgery: {surgery.value}')\n",
    "    print(sep)\n",
    "\n",
    "    for transcription in transcriptions:\n",
    "        messages = builder.build_messages(transcription, snapshot=None)\n",
    "\n",
    "        t0 = time.time()\n",
    "        response = llm.create_chat_completion(\n",
    "            messages    = messages,\n",
    "            max_tokens  = 256,\n",
    "            temperature = 0.1,\n",
    "            top_p       = 0.9,\n",
    "        )\n",
    "        latency_ms = (time.time() - t0) * 1000\n",
    "\n",
    "        raw_text = response['choices'][0]['message']['content']\n",
    "        parsed   = parse_llm_output(raw_text, surgery)\n",
    "\n",
    "        results.append({\n",
    "            'surgery':       surgery.value,\n",
    "            'transcription': transcription,\n",
    "            'raw_output':    raw_text,\n",
    "            'parsed_on':     parsed.machine_states['1'],\n",
    "            'parsed_off':    parsed.machine_states['0'],\n",
    "            'reasoning':     parsed.reasoning,\n",
    "            'latency_ms':    latency_ms,\n",
    "        })\n",
    "\n",
    "        reasoning_preview = (parsed.reasoning[:80] + '...') if len(parsed.reasoning) > 80 else parsed.reasoning\n",
    "        print(f'\\nTranscription : {transcription}')\n",
    "        print(f'ON            : {parsed.machine_states[\"1\"]}')\n",
    "        print(f'OFF           : {parsed.machine_states[\"0\"]}')\n",
    "        print(f'Reasoning     : {reasoning_preview}')\n",
    "        print(f'Latency       : {latency_ms:.0f} ms')\n",
    "\n",
    "print(f'\\nTotal inference calls: {len(results)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca0b9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 9. Latency summary ───────────────────────────────────────────────────\n",
    "import statistics\n",
    "\n",
    "latencies = [r['latency_ms'] for r in results]\n",
    "print('Latency Statistics (ms):')\n",
    "print(f'  Min    : {min(latencies):.0f}')\n",
    "print(f'  Max    : {max(latencies):.0f}')\n",
    "print(f'  Mean   : {statistics.mean(latencies):.0f}')\n",
    "print(f'  Median : {statistics.median(latencies):.0f}')\n",
    "print(f'  P95    : {sorted(latencies)[int(len(latencies)*0.95)]:.0f}')\n",
    "\n",
    "target_ms = 3000\n",
    "under_target = sum(1 for lat in latencies if lat < target_ms)\n",
    "print(f'\\nUnder {target_ms}ms target: {under_target}/{len(latencies)} ({100*under_target/len(latencies):.0f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5395587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 10. Correctness check ────────────────────────────────────────────────\n",
    "# Heuristic: if keyword in transcription → expect that canonical machine to be ON\n",
    "# (skips \"turn off X\" sentences and any transcription with no matching keyword)\n",
    "keyword_to_canonical = {\n",
    "    'ventilator':       'Ventilator',\n",
    "    'bypass pump':      'Cardiopulmonary Bypass Machine',\n",
    "    'defibrillator':    'Defibrillator',\n",
    "    'or lights':        'Surgical Lights',\n",
    "    'cardiac monitor':  'Patient Monitor',\n",
    "    'perfusion pump':   'Perfusion Pump',\n",
    "    'warming blanket':  'Warming Blanket',\n",
    "    'laparoscopic':     'Laparoscopic Tower',\n",
    "    'ultrasonic':       'Ultrasonic Dissector',\n",
    "    'lithotripter':     'Lithotripter',\n",
    "    'c-arm':            'Fluoroscopy C-Arm',\n",
    "    'fluoroscop':       'Fluoroscopy C-Arm',\n",
    "    'nephroscope':      'Nephroscope Tower',\n",
    "    'irrigation pump':  'Irrigation Pump',\n",
    "    'suction':          'Suction Unit',\n",
    "}\n",
    "\n",
    "correct = incorrect = skipped = 0\n",
    "\n",
    "for r in results:\n",
    "    tr  = r['transcription'].lower()\n",
    "    on  = [m.lower() for m in r['parsed_on']]\n",
    "\n",
    "    hit_kw = False\n",
    "    for kw, expected in keyword_to_canonical.items():\n",
    "        if kw in tr:\n",
    "            hit_kw = True\n",
    "            # Skip if this keyword appears after a \"turn off\" / \"off\" command\n",
    "            idx = tr.find(kw)\n",
    "            prefix = tr[max(0, idx-15):idx]\n",
    "            if 'off' in prefix:\n",
    "                skipped += 1\n",
    "            elif any(expected.lower() in o for o in on):\n",
    "                correct += 1\n",
    "            else:\n",
    "                incorrect += 1\n",
    "                print(f'MISS: \"{r[\"transcription\"]}\"')\n",
    "                print(f'      expected ON: {expected!r}, got: {r[\"parsed_on\"]}')\n",
    "            break\n",
    "\n",
    "    if not hit_kw:\n",
    "        skipped += 1\n",
    "\n",
    "checked = correct + incorrect\n",
    "if checked > 0:\n",
    "    print(f'\\nHeuristic correctness : {correct}/{checked} ({100*correct/checked:.0f}%)')\n",
    "    print(f'Skipped (no heuristic): {skipped}')\n",
    "else:\n",
    "    print('No keyword-matched heuristics to evaluate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779323e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 11. Save results to JSON ─────────────────────────────────────────────\n",
    "from pathlib import Path\n",
    "\n",
    "output_path = Path('/kaggle/working/phase3_results.json')\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "print(f'Results saved to {output_path}')\n",
    "print(f'Total calls  : {len(results)}')\n",
    "print(f'Avg latency  : {statistics.mean(latencies):.0f} ms')\n",
    "print(f'Median       : {statistics.median(latencies):.0f} ms')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0b99ce",
   "metadata": {},
   "source": [
    "## Phase 3 Sign-off Criteria\n",
    "\n",
    "| Check | Target | How verified |\n",
    "|-------|--------|-------------|\n",
    "| MedGemma loads without error | ✅ no exception | Cell 6 |\n",
    "| `llama_supports_gpu_offload()` returns `True` | ✅ must be True | Cell 2 |\n",
    "| All 30 JSON responses parse successfully | 30 / 30 non-empty `machine_states` | Cell 8 |\n",
    "| Median inference latency | < 3 000 ms | Cell 9 |\n",
    "| Heuristic keyword correctness | > 80 % | Cell 10 |\n",
    "\n",
    "If all pass → Phase 3 complete → report results → **proceed to Phase 4: E2E Pipeline**  \n",
    "(`LiveTranscriber → MedGemmaModel → StateManager → machine_states.json`)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
