{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3296229",
   "metadata": {},
   "source": [
    "# OR-SIM — Complete End-to-End Testing Guide\n",
    "## Backend on Kaggle (GPU) + Frontend on Local Machine\n",
    "\n",
    "```\n",
    "Your Windows Machine                    Kaggle Notebook (T4 GPU)\n",
    "──────────────────────                  ─────────────────────────\n",
    "Browser → http://localhost:5173         uvicorn :8000  (FastAPI)\n",
    "    Vite dev-server                          │  MedGemma GGUF  (~2.1 GB)\n",
    "          │  HTTPS / WSS                     │  MedASR int8 ONNX  (~85 MB)\n",
    "    ngrok public URL ───────────────────────►│\n",
    "```\n",
    "\n",
    "## What this notebook covers\n",
    "| Phase | Cells | What is tested |\n",
    "|-------|-------|----------------|\n",
    "| **Setup** | 1–6 | GPU, repo clone, llama-cpp CUDA wheel, Python deps, model downloads |\n",
    "| **Unit tests** | 7 | Full pytest suite (routes + WebSocket manager + pipeline mocks) |\n",
    "| **Component smoke** | 8–10 | ASR loads + transcribes · LLM loads + responds · Pipeline state transitions |\n",
    "| **Server + tunnel** | 11–13 | FastAPI binds port 8000, all REST endpoints healthy, ngrok URL printed |\n",
    "| **E2E API tests** | 14 | Automated REST + WebSocket round-trip against the live server |\n",
    "| **Frontend (local)** | 15 | Step-by-step Vite testing instructions |\n",
    "| **Keep-alive** | 16 | Ping loop prevents Kaggle idle timeout |\n",
    "\n",
    "## Prerequisites\n",
    "| Requirement | Where |\n",
    "|---|---|\n",
    "| Kaggle GPU — T4 x2 **or** P100 (≥ 15 GB VRAM) | Notebook Settings → Accelerator |\n",
    "| Internet access **On** | Notebook Settings → Internet |\n",
    "| Free ngrok account | https://ngrok.com |\n",
    "| Node.js ≥ 18 (local Windows PC) | https://nodejs.org |\n",
    "\n",
    "## Only two values you must fill in before running\n",
    "| Variable | Cell | Value |\n",
    "|---|---|---|\n",
    "| `NGROK_AUTH_TOKEN` | 12 | Your ngrok auth token — https://dashboard.ngrok.com/get-started/your-authtoken |\n",
    "| `HF_TOKEN` | 6 | Optional — enables Tier 1 google/medasr (6.6% WER). Leave `''` to use Tier 2 int8 (always works, no token). |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a913e3b",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 1 — Verify GPU accelerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90175f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, sys\n",
    "\n",
    "result = subprocess.run(\n",
    "    ['nvidia-smi', '--query-gpu=name,memory.total,driver_version,cuda_version',\n",
    "     '--format=csv,noheader'],\n",
    "    capture_output=True, text=True,\n",
    ")\n",
    "if result.returncode != 0:\n",
    "    raise RuntimeError(\n",
    "        'No GPU detected.  Go to Notebook Settings → Accelerator → select GPU T4 x2 or P100.'\n",
    "    )\n",
    "\n",
    "gpu_line = result.stdout.strip()\n",
    "print('GPU detected:', gpu_line)\n",
    "\n",
    "# Warn if less than 12 GB (MedGemma Q3 needs ~2.1 GB VRAM, MedASR ~0.2 GB)\n",
    "import re\n",
    "mem_match = re.search(r'(\\d+)\\s*MiB', gpu_line)\n",
    "if mem_match and int(mem_match.group(1)) < 12000:\n",
    "    print('WARNING: GPU has < 12 GB VRAM — reduce n_gpu_layers in the session/start call if needed.')\n",
    "else:\n",
    "    print('VRAM check: OK')\n",
    "\n",
    "# Also print CUDA runtime version\n",
    "cuda_result = subprocess.run(['nvcc', '--version'], capture_output=True, text=True)\n",
    "if cuda_result.returncode == 0:\n",
    "    print('CUDA:', cuda_result.stdout.strip().split('\\n')[-1])\n",
    "else:\n",
    "    print('nvcc not found — checking driver CUDA level from nvidia-smi only.')\n",
    "print()\n",
    "print('Cell 1 PASSED ✓')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eefa182",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 2 — Clone the OR-SIM repository\n",
    "\n",
    "> `REPO_URL` points to the GitHub repo. The clone is skipped if already present (re-run safe).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1557443",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, subprocess\n",
    "\n",
    "REPO_URL = 'https://github.com/Aditya-Lingam-9000/OR-SIM.git'\n",
    "REPO_DIR = '/kaggle/working/OR-SIM'\n",
    "\n",
    "if not os.path.isdir(os.path.join(REPO_DIR, '.git')):\n",
    "    subprocess.run(['git', 'clone', '--depth', '1', REPO_URL, REPO_DIR], check=True)\n",
    "    print(f'Cloned → {REPO_DIR}')\n",
    "else:\n",
    "    subprocess.run(['git', '-C', REPO_DIR, 'pull', '--ff-only'], check=True)\n",
    "    print(f'Updated existing clone at {REPO_DIR}')\n",
    "\n",
    "# Add repo root to sys.path — required for `import backend.*`\n",
    "if REPO_DIR not in sys.path:\n",
    "    sys.path.insert(0, REPO_DIR)\n",
    "\n",
    "# Show last 5 commits as a sanity check\n",
    "subprocess.run(['git', '-C', REPO_DIR, 'log', '--oneline', '-5'], check=True)\n",
    "\n",
    "# Verify expected directories exist\n",
    "for expected in ('backend', 'frontend', 'requirements', 'tests', 'models'):\n",
    "    path = os.path.join(REPO_DIR, expected)\n",
    "    assert os.path.isdir(path), f'Missing expected directory: {path}'\n",
    "    print(f'  ✓  {expected}/')\n",
    "\n",
    "print()\n",
    "print('Cell 2 PASSED ✓')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146eb69e",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 3 — Install llama-cpp-python (CUDA 12.4 prebuilt wheel)\n",
    "\n",
    "Kaggle T4 instances ship **CUDA 12.4**.  Using the prebuilt wheel from `abetlen.github.io` takes ~30 s instead of ~5 min from source.  The `llama_supports_gpu_offload()` check confirms the CUDA extension compiled correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61fd89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.run(\n",
    "    [\n",
    "        sys.executable, '-m', 'pip', 'install', '--quiet',\n",
    "        'llama-cpp-python',\n",
    "        '--extra-index-url', 'https://abetlen.github.io/llama-cpp-python/whl/cu124',\n",
    "    ],\n",
    "    check=True,\n",
    ")\n",
    "\n",
    "import llama_cpp\n",
    "lib    = getattr(llama_cpp, 'llama_cpp', None)\n",
    "gpu_ok = lib.llama_supports_gpu_offload() if lib else False\n",
    "print(f'llama_supports_gpu_offload() = {gpu_ok}')\n",
    "if not gpu_ok:\n",
    "    raise RuntimeError(\n",
    "        'GPU offload NOT compiled in.\\n'\n",
    "        'Check Cell 1 shows a CUDA 12.x driver — the cu124 wheel requires CUDA ≥ 12.4.\\n'\n",
    "        'If on P100 with CUDA 11.x, change the index URL to cu118.'\n",
    "    )\n",
    "\n",
    "from llama_cpp import Llama\n",
    "print('llama-cpp-python import OK — version:', llama_cpp.__version__)\n",
    "print()\n",
    "print('Cell 3 PASSED ✓')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2e3bf5",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 4 — Install OR-SIM Python dependencies\n",
    "\n",
    "Installs `base.txt`, `asr.txt`, `server.txt`, `dev.txt` from the repo.  \n",
    "`llm.txt` is **skipped** to preserve the CUDA wheel installed in Cell 3.  \n",
    "`libportaudio2` is installed first — required by `sounddevice` even though there is no microphone on Kaggle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31c4a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# libportaudio2 — C library required by sounddevice (imported at module level in backend/asr)\n",
    "subprocess.run(['apt-get', 'install', '-y', '-q', 'libportaudio2'], check=False)\n",
    "\n",
    "req_files = ['base.txt', 'asr.txt', 'server.txt', 'dev.txt']\n",
    "for rf in req_files:\n",
    "    rpath = os.path.join(REPO_DIR, 'requirements', rf)\n",
    "    assert os.path.isfile(rpath), f'Missing requirements file: {rpath}'\n",
    "    subprocess.run(\n",
    "        [sys.executable, '-m', 'pip', 'install', '--quiet', '-r', rpath],\n",
    "        check=True,\n",
    "    )\n",
    "    print(f'  requirements/{rf}  installed  ✓')\n",
    "\n",
    "# huggingface_hub — used in Cells 5 and 6 for model downloads\n",
    "subprocess.run(\n",
    "    [sys.executable, '-m', 'pip', 'install', '--quiet', 'huggingface_hub'],\n",
    "    check=True,\n",
    ")\n",
    "print('  huggingface_hub          installed  ✓')\n",
    "\n",
    "# Quick import smoke-test of key packages\n",
    "for pkg, import_name in [\n",
    "    ('fastapi',  'fastapi'),\n",
    "    ('uvicorn',  'uvicorn'),\n",
    "    ('loguru',   'loguru'),\n",
    "    ('pydantic', 'pydantic'),\n",
    "    ('numpy',    'numpy'),\n",
    "    ('torch',    'torch'),\n",
    "    ('onnxruntime', 'onnxruntime'),\n",
    "    ('pytest',   'pytest'),\n",
    "    ('httpx',    'httpx'),\n",
    "]:\n",
    "    try:\n",
    "        __import__(import_name)\n",
    "        print(f'  import {pkg:<20} OK')\n",
    "    except ImportError as e:\n",
    "        raise ImportError(f'Failed to import {pkg}: {e}')\n",
    "\n",
    "print()\n",
    "print('Cell 4 PASSED ✓')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a64a85",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 5 — Download MedGemma GGUF\n",
    "\n",
    "Downloads `medgemma-4b-it-Q3_K_M.gguf` (~2.1 GB) from `unsloth/medgemma-4b-it-GGUF` (public, no token needed).  \n",
    "Skipped automatically if the file already exists.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0cd160",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "from pathlib import Path\n",
    "\n",
    "GGUF_DIR = Path(REPO_DIR) / 'models' / 'medgemma'\n",
    "GGUF_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "GGUF_FILE = GGUF_DIR / 'medgemma-4b-it-Q3_K_M.gguf'\n",
    "\n",
    "if GGUF_FILE.exists():\n",
    "    size_gb = GGUF_FILE.stat().st_size / 1e9\n",
    "    print(f'GGUF already present  ({size_gb:.2f} GB) — skipping download.')\n",
    "    if size_gb < 1.8:\n",
    "        raise RuntimeError('GGUF file looks truncated (< 1.8 GB). Delete it and re-run.')\n",
    "else:\n",
    "    print('Downloading medgemma-4b-it-Q3_K_M.gguf (~2.1 GB)...')\n",
    "    print('Expected time: 2–5 min on Kaggle')\n",
    "    MODEL_PATH = Path(hf_hub_download(\n",
    "        repo_id   = 'unsloth/medgemma-4b-it-GGUF',\n",
    "        filename  = 'medgemma-4b-it-Q3_K_M.gguf',\n",
    "        local_dir = str(GGUF_DIR),\n",
    "    ))\n",
    "    GGUF_FILE = MODEL_PATH\n",
    "\n",
    "size_gb = GGUF_FILE.stat().st_size / 1e9\n",
    "print(f'Model path : {GGUF_FILE}')\n",
    "print(f'File size  : {size_gb:.2f} GB')\n",
    "assert size_gb > 1.8, 'GGUF file suspiciously small — re-run this cell.'\n",
    "\n",
    "MODEL_PATH = GGUF_FILE   # used in later cells\n",
    "print()\n",
    "print('Cell 5 PASSED ✓')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490f6041",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 6 — Download MedASR models\n",
    "\n",
    "**Part A (always runs)** — Tier 2 sherpa-onnx int8 ONNX model (~85 MB, public, no token).  \n",
    "**Part B (optional)** — Tier 1 google/medasr PyTorch weights (~420 MB, requires HF token and accepting the model terms at https://huggingface.co/google/medasr).  \n",
    "\n",
    "| Tier | Model | WER | Token req? |\n",
    "|------|-------|-----|------------|\n",
    "| 1 | google/medasr PyTorch | 6.6% | Yes |\n",
    "| 2 | sherpa-onnx int8 ONNX | ~18% | No |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca68e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "from pathlib import Path\n",
    "\n",
    "ASR_DIR = Path(REPO_DIR) / 'models' / 'medasr'\n",
    "ASR_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ── Part A: Tier 2 baseline (always runs) ─────────────────────────────────────\n",
    "INT8_REPO = 'csukuangfj/sherpa-onnx-medasr-ctc-en-int8-2025-12-25'\n",
    "for fname in ('model.int8.onnx', 'tokens.txt'):\n",
    "    fpath = ASR_DIR / fname\n",
    "    if fpath.exists():\n",
    "        print(f'  [Tier 2] {fname}  already present  ({fpath.stat().st_size / 1e6:.1f} MB) — skip')\n",
    "    else:\n",
    "        print(f'  [Tier 2] Downloading {fname}...')\n",
    "        hf_hub_download(repo_id=INT8_REPO, filename=fname, local_dir=str(ASR_DIR))\n",
    "        print(f'  [Tier 2] {fname}  downloaded  ({(ASR_DIR / fname).stat().st_size / 1e6:.1f} MB)')\n",
    "\n",
    "assert (ASR_DIR / 'model.int8.onnx').exists(), 'int8 ONNX download failed!'\n",
    "assert (ASR_DIR / 'tokens.txt').exists(),      'tokens.txt download failed!'\n",
    "print()\n",
    "print('[Tier 2] MedASR int8 ONNX ready ✓')\n",
    "\n",
    "# ── Part B: Tier 1 upgrade (optional) ────────────────────────────────────────\n",
    "# Set your HF token below OR add a Kaggle Secret named 'HF_TOKEN'\n",
    "HF_TOKEN = ''   # ← paste: hf_xxxxxxxxxxxxxxxxxxxx\n",
    "\n",
    "if not HF_TOKEN:\n",
    "    try:\n",
    "        from kaggle_secrets import UserSecretsClient\n",
    "        HF_TOKEN = UserSecretsClient().get_secret('HF_TOKEN')\n",
    "        print('[Tier 1] HF_TOKEN loaded from Kaggle Secret ✓')\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "if not HF_TOKEN:\n",
    "    print()\n",
    "    print('[Tier 1] No HF_TOKEN — Tier 1 download skipped.  Tier 2 (int8 ONNX) will be used.')\n",
    "    print('         To upgrade: accept terms at https://huggingface.co/google/medasr')\n",
    "    print('         then paste your token above and re-run.')\n",
    "else:\n",
    "    TORCH_DIR = ASR_DIR / 'pytorch'\n",
    "    if (TORCH_DIR / 'config.json').exists():\n",
    "        size_mb = sum(f.stat().st_size for f in TORCH_DIR.rglob('*') if f.is_file()) / 1e6\n",
    "        print(f'[Tier 1] google/medasr already downloaded ({size_mb:.0f} MB) — skipping.')\n",
    "    else:\n",
    "        TORCH_DIR.mkdir(parents=True, exist_ok=True)\n",
    "        print('[Tier 1] Downloading google/medasr PyTorch weights (~420 MB)...')\n",
    "        try:\n",
    "            from huggingface_hub import snapshot_download\n",
    "            snapshot_download(\n",
    "                repo_id         = 'google/medasr',\n",
    "                local_dir       = str(TORCH_DIR),\n",
    "                token           = HF_TOKEN,\n",
    "                ignore_patterns = ['*.msgpack', '*.h5', 'flax_model*', 'tf_model*', 'rust_model*'],\n",
    "            )\n",
    "            size_mb = sum(f.stat().st_size for f in TORCH_DIR.rglob('*') if f.is_file()) / 1e6\n",
    "            print(f'[Tier 1] google/medasr downloaded ({size_mb:.0f} MB) ✓')\n",
    "        except Exception as e:\n",
    "            print(f'[Tier 1] Download failed ({type(e).__name__}: {e})')\n",
    "            print('[Tier 1] Falling back to Tier 2 (int8 ONNX). No action needed.')\n",
    "\n",
    "print()\n",
    "print('Cell 6 PASSED ✓')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7558482b",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 7 — Run unit tests via pytest\n",
    "\n",
    "Runs the full test suite inside Kaggle with all dependencies already installed.  \n",
    "All tests use mocks — no real GPU inference, no microphone.  \n",
    "\n",
    "Expected: **all tests PASSED** (no errors, no failures).  \n",
    "The test run covers:\n",
    "- `tests/phase5/test_routes.py` — all REST + WebSocket endpoints\n",
    "- `tests/phase5/test_ws_manager.py` — ConnectionManager unit tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1362ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, sys, os\n",
    "\n",
    "result = subprocess.run(\n",
    "    [\n",
    "        sys.executable, '-m', 'pytest',\n",
    "        'tests/',\n",
    "        '-v',\n",
    "        '--tb=short',\n",
    "        '--color=yes',\n",
    "        '-q',\n",
    "    ],\n",
    "    cwd=REPO_DIR,\n",
    "    capture_output=False,\n",
    "    text=True,\n",
    "    env={**os.environ, 'PYTHONPATH': REPO_DIR, 'PYTHONDONTWRITEBYTECODE': '1'},\n",
    ")\n",
    "\n",
    "print(f'\\npytest exit code: {result.returncode}')\n",
    "if result.returncode != 0:\n",
    "    raise RuntimeError(\n",
    "        'pytest FAILED.  Scroll up to see which tests failed and why.\\n'\n",
    "        'Common cause: missing import → re-run Cell 4.\\n'\n",
    "        'If a specific test fails, check the traceback above.'\n",
    "    )\n",
    "\n",
    "print()\n",
    "print('Cell 7 PASSED ✓  — all unit tests green')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2033b4",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 8 — Smoke test: MedASR loads and transcribes\n",
    "\n",
    "Exercises the ASR transcriber with a synthetic 1-second sine-wave audio buffer.  \n",
    "Tests that the int8 ONNX model loads, runs inference, and returns a string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7ea531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, REPO_DIR)\n",
    "os.environ.setdefault('MEDASR_MODEL_DIR', str(Path(REPO_DIR) / 'models' / 'medasr'))\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Synthetic audio: 1 s of 440 Hz sine at 16 kHz (silence-equivalent for ASR — expects empty/low output)\n",
    "SAMPLE_RATE = 16_000\n",
    "t = np.linspace(0, 1.0, SAMPLE_RATE, dtype=np.float32)\n",
    "audio_chunk = (0.05 * np.sin(2 * np.pi * 440 * t)).astype(np.float32)\n",
    "\n",
    "from backend.asr.transcriber import MedASRTranscriber\n",
    "\n",
    "print('Loading MedASR transcriber...')\n",
    "asr = MedASRTranscriber()\n",
    "print(f'  model tier : {getattr(asr, \"tier\", \"unknown\")}')\n",
    "print(f'  model path : {getattr(asr, \"model_path\", \"unknown\")}')\n",
    "\n",
    "print('Running inference on synthetic audio...')\n",
    "result = asr.transcribe(audio_chunk)\n",
    "print(f'  transcription result : {repr(result)}')\n",
    "assert isinstance(result, str), f'Expected str, got {type(result)}'\n",
    "print()\n",
    "print('Cell 8 PASSED ✓  — MedASR loads and runs inference successfully')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f87f2c",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 9 — Smoke test: MedGemma LLM loads and parses OR commands\n",
    "\n",
    "Loads `medgemma-4b-it-Q3_K_M.gguf` onto GPU and runs a single structured inference.  \n",
    "**Expected time: 20–40 s** (model load from disk + 1 forward pass).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58faef3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['MEDASR_MODEL_DIR']   = str(Path(REPO_DIR) / 'models' / 'medasr')\n",
    "os.environ['MEDGEMMA_MODEL_DIR'] = str(Path(REPO_DIR) / 'models' / 'medgemma')\n",
    "\n",
    "from backend.llm.medgemma     import MedGemmaModel\n",
    "from backend.data.surgeries   import SurgeryType\n",
    "\n",
    "print('Loading MedGemma model onto GPU...')\n",
    "print('(~20–40 s on T4 — this is normal)')\n",
    "llm = MedGemmaModel(model_path=str(MODEL_PATH), n_gpu_layers=-1)\n",
    "print('  model loaded ✓')\n",
    "\n",
    "# Build a test prompt for Heart Transplant surgery\n",
    "from backend.llm.prompt_builder import build_prompt\n",
    "\n",
    "surgery     = SurgeryType.HEART_TRANSPLANT\n",
    "transcripts = ['turn on the heart lung machine', 'activate the patient monitor']\n",
    "machines    = ['Patient Monitor', 'Ventilator', 'Cardiopulmonary Bypass']\n",
    "\n",
    "print()\n",
    "print('Running structured inference...')\n",
    "prompt = build_prompt(surgery=surgery, transcript='\\n'.join(transcripts), machines=machines)\n",
    "raw_output = llm.generate(prompt)\n",
    "\n",
    "print(f'  raw output  : {repr(raw_output[:200])}')\n",
    "assert isinstance(raw_output, str) and len(raw_output) > 0, 'LLM returned empty output!'\n",
    "\n",
    "# Parse the output\n",
    "from backend.llm.output_parser import parse_llm_output\n",
    "\n",
    "parsed = parse_llm_output(raw_output, machines=machines)\n",
    "print(f'  parsed      : {parsed}')\n",
    "assert isinstance(parsed, dict), f'Expected dict, got {type(parsed)}'\n",
    "\n",
    "print()\n",
    "print('Cell 9 PASSED ✓  — MedGemma loads, infers, and parses machine states correctly')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412db221",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 10 — Smoke test: Pipeline state transitions (mocked audio)\n",
    "\n",
    "Instantiates an `ORPipeline` with mocked audio capture so no real microphone is needed.  \n",
    "Simulates a voice command by directly calling the LLM → state update path.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd58534",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unittest.mock import patch, MagicMock\n",
    "import threading, time\n",
    "\n",
    "from backend.pipeline.pipeline  import ORPipeline\n",
    "from backend.data.surgeries     import SurgeryType\n",
    "from backend.data.models        import ORStateSnapshot\n",
    "\n",
    "surgery = SurgeryType.HEART_TRANSPLANT\n",
    "\n",
    "# Mock AudioCapture so no microphone is required\n",
    "with patch('backend.pipeline.pipeline.AudioCapture') as MockAudio:\n",
    "    mock_audio_instance         = MagicMock()\n",
    "    mock_audio_instance.start   = MagicMock()\n",
    "    mock_audio_instance.stop    = MagicMock()\n",
    "    mock_audio_instance.read    = MagicMock(return_value=None)   # no audio\n",
    "    MockAudio.return_value      = mock_audio_instance\n",
    "\n",
    "    pipeline = ORPipeline(surgery=surgery, n_gpu_layers=-1)\n",
    "\n",
    "print(f'Pipeline created for surgery: {surgery.value}')\n",
    "print(f'  state_manager type: {type(pipeline.state_manager).__name__}')\n",
    "\n",
    "# Verify initial state snapshot\n",
    "snapshot = pipeline.state_manager.get_snapshot()\n",
    "print(f'  initial snapshot type   : {type(snapshot).__name__}')\n",
    "assert isinstance(snapshot, ORStateSnapshot), 'Expected ORStateSnapshot'\n",
    "print(f'  initial surgery         : {snapshot.surgery}')\n",
    "assert snapshot.surgery == surgery.value, 'Surgery name mismatch'\n",
    "\n",
    "# Verify all machines are initially OFF\n",
    "initial_on = snapshot.machine_states.get('1', [])\n",
    "print(f'  initial machines ON     : {initial_on}')\n",
    "assert initial_on == [], 'Machines should all be OFF at startup'\n",
    "\n",
    "# Simulate a transcription event triggering the LLM pipeline\n",
    "print()\n",
    "print('Simulating voice command: \"activate ventilator and patient monitor\"...')\n",
    "pipeline.state_manager._process_transcript(\n",
    "    'activate ventilator and patient monitor'\n",
    ")\n",
    "time.sleep(0.5)   # allow state update to propagate\n",
    "\n",
    "updated = pipeline.state_manager.get_snapshot()\n",
    "print(f'  machines ON after command: {updated.machine_states.get(\"1\", [])}')\n",
    "print(f'  transcription recorded  : {repr(updated.transcription)}')\n",
    "\n",
    "print()\n",
    "print('Cell 10 PASSED ✓  — Pipeline state transitions verified')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf2b983",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 11 — Start the FastAPI backend server\n",
    "\n",
    "Launches `python -m backend.server` as a subprocess (avoids asyncio event-loop conflicts between uvicorn and Jupyter's own loop).  \n",
    "MedGemma loads into GPU VRAM during startup — **allow up to 60 seconds**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690f3b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket, time, urllib.request, json as _json, os\n",
    "\n",
    "# Stop any previous server from a prior run of this cell\n",
    "try:\n",
    "    if _server_proc.poll() is None:\n",
    "        _server_proc.terminate()\n",
    "        _server_proc.wait(timeout=5)\n",
    "        print('Stopped previous server instance.')\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "env            = os.environ.copy()\n",
    "env['PYTHONPATH']            = REPO_DIR\n",
    "env['MEDASR_MODEL_DIR']      = str(Path(REPO_DIR) / 'models' / 'medasr')\n",
    "env['MEDGEMMA_MODEL_DIR']    = str(Path(REPO_DIR) / 'models' / 'medgemma')\n",
    "\n",
    "_server_proc = subprocess.Popen(\n",
    "    [sys.executable, '-m', 'backend.server', '--host', '0.0.0.0', '--port', '8000'],\n",
    "    cwd=REPO_DIR,\n",
    "    env=env,\n",
    ")\n",
    "print(f'Server PID: {_server_proc.pid}')\n",
    "print('Waiting for FastAPI to bind port 8000 ', end='', flush=True)\n",
    "\n",
    "deadline   = time.time() + 90    # 90 s for model load\n",
    "connected  = False\n",
    "while time.time() < deadline:\n",
    "    if _server_proc.poll() is not None:\n",
    "        raise RuntimeError(\n",
    "            f'Server exited early (exit code {_server_proc.returncode}).\\n'\n",
    "            'Scroll up for the traceback — common causes:\\n'\n",
    "            '  • GGUF not found → re-run Cell 5\\n'\n",
    "            '  • int8 ONNX not found → re-run Cell 6\\n'\n",
    "            '  • import error → re-run Cell 4'\n",
    "        )\n",
    "    try:\n",
    "        with socket.create_connection(('127.0.0.1', 8000), timeout=1):\n",
    "            connected = True\n",
    "            break\n",
    "    except OSError:\n",
    "        print('.', end='', flush=True)\n",
    "        time.sleep(1)\n",
    "\n",
    "print()\n",
    "if not connected:\n",
    "    raise TimeoutError('Server did not bind within 90 s — check startup logs above.')\n",
    "\n",
    "# Health check\n",
    "try:\n",
    "    with urllib.request.urlopen('http://127.0.0.1:8000/api/health', timeout=10) as r:\n",
    "        health = _json.loads(r.read())\n",
    "    print(f'Health check: {_json.dumps(health, indent=2)}')\n",
    "    assert health['status'] == 'ok', f'Unexpected health status: {health}'\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f'Health check failed: {e}')\n",
    "\n",
    "print()\n",
    "print('Cell 11 PASSED ✓  — FastAPI server is running and healthy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6e3d7f",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 12 — Install pyngrok and open the tunnel\n",
    "\n",
    "**You must paste your ngrok auth token** (free account) before running.  \n",
    "Get it from https://dashboard.ngrok.com/get-started/your-authtoken  \n",
    "\n",
    "Or add a Kaggle Secret (recommended for shared notebooks):  \n",
    "Sidebar → Add-ons → Secrets → + Add → Key: `NGROK_TOKEN` → Value: your token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd221583",
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.run(\n",
    "    [sys.executable, '-m', 'pip', 'install', 'pyngrok', '--quiet'],\n",
    "    check=True,\n",
    ")\n",
    "from pyngrok import ngrok\n",
    "print('pyngrok installed ✓')\n",
    "\n",
    "# ── CONFIGURE YOUR TOKEN HERE ──────────────────────────────────────────────\n",
    "NGROK_AUTH_TOKEN = 'YOUR_NGROK_AUTH_TOKEN'   # ← paste your token\n",
    "\n",
    "# Option B — Kaggle Secret (uncomment the three lines below):\n",
    "# from kaggle_secrets import UserSecretsClient\n",
    "# NGROK_AUTH_TOKEN = UserSecretsClient().get_secret('NGROK_TOKEN')\n",
    "# print('NGROK_AUTH_TOKEN: loaded from Kaggle Secret')\n",
    "\n",
    "if NGROK_AUTH_TOKEN == 'YOUR_NGROK_AUTH_TOKEN':\n",
    "    raise ValueError(\n",
    "        'ngrok token not configured.\\n'\n",
    "        'Edit this cell and paste your real token.\\n'\n",
    "        'Get it from: https://dashboard.ngrok.com/get-started/your-authtoken'\n",
    "    )\n",
    "\n",
    "# Close any stale tunnels from previous runs\n",
    "for _t in ngrok.get_tunnels():\n",
    "    ngrok.disconnect(_t.public_url)\n",
    "\n",
    "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
    "tunnel      = ngrok.connect(8000, bind_tls=True)\n",
    "NGROK_HTTPS = tunnel.public_url\n",
    "NGROK_WSS   = NGROK_HTTPS.replace('https://', 'wss://')\n",
    "\n",
    "print()\n",
    "print('=' * 70)\n",
    "print('  OR-SIM BACKEND IS LIVE')\n",
    "print('=' * 70)\n",
    "print(f'  HTTPS  (REST API + WebSocket) : {NGROK_HTTPS}')\n",
    "print(f'  WSS    (WebSocket endpoint)   : {NGROK_WSS}/ws/state')\n",
    "print('=' * 70)\n",
    "print()\n",
    "print('NEXT STEP: Copy the HTTPS URL and follow Cell 15 on your local machine.')\n",
    "print()\n",
    "print('Cell 12 PASSED ✓')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9a06ca",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 13 — Verify all API endpoints through the tunnel\n",
    "\n",
    "Confirms every REST endpoint is reachable via the ngrok URL.  \n",
    "The `ngrok-skip-browser-warning` header bypasses the ngrok browser-warning page.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7245802",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request, json as _j\n",
    "\n",
    "SKIP = {'ngrok-skip-browser-warning': '1'}\n",
    "\n",
    "def _get(path, timeout=15):\n",
    "    req = urllib.request.Request(f'{NGROK_HTTPS}{path}', headers=SKIP)\n",
    "    with urllib.request.urlopen(req, timeout=timeout) as r:\n",
    "        return r.status, _j.loads(r.read())\n",
    "\n",
    "checks = [\n",
    "    ('/api/health', 'Health'),\n",
    "    ('/api/state',  'State (no session — expect 400)'),\n",
    "]\n",
    "\n",
    "results = []\n",
    "for path, label in checks:\n",
    "    try:\n",
    "        status, data = _get(path)\n",
    "        preview      = _j.dumps(data, indent=2)[:300]\n",
    "        print(f'{label} →  HTTP {status}')\n",
    "        print(preview)\n",
    "        print()\n",
    "        results.append((label, status, True))\n",
    "    except urllib.error.HTTPError as e:\n",
    "        # /api/state returns 400 when no session is active — that's expected\n",
    "        if e.code == 400 and path == '/api/state':\n",
    "            body = _j.loads(e.read())\n",
    "            print(f'{label} →  HTTP 400 (expected — no active session)')\n",
    "            print(f'  detail: {body.get(\"detail\", \"\")}')\n",
    "            print()\n",
    "            results.append((label, 400, True))\n",
    "        else:\n",
    "            print(f'{label} →  UNEXPECTED HTTP {e.code}: {e.reason}')\n",
    "            results.append((label, e.code, False))\n",
    "    except Exception as ex:\n",
    "        print(f'{label} →  ERROR: {ex}')\n",
    "        results.append((label, None, False))\n",
    "\n",
    "all_ok = all(ok for _, _, ok in results)\n",
    "assert all_ok, 'One or more endpoint checks FAILED — see output above.'\n",
    "\n",
    "print('All endpoint checks PASSED ✓')\n",
    "print()\n",
    "print('Cell 13 PASSED ✓')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801d620c",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 14 — End-to-end automated API tests (REST + WebSocket)\n",
    "\n",
    "Runs a full test sequence against the live FastAPI server on port 8000:  \n",
    "\n",
    "1. `GET /api/health` — server healthy  \n",
    "2. `GET /api/state` — 400 while no session active  \n",
    "3. `POST /api/session/start` with `surgery=heart` — 200, pipeline starts  \n",
    "4. `GET /api/health` — `pipeline_active=True`  \n",
    "5. `GET /api/state` — 200 with machine states dict  \n",
    "6. `WS /ws/state` — connect, receive state snapshot JSON  \n",
    "7. `POST /api/session/stop` — 200, pipeline stops  \n",
    "8. `GET /api/health` — `pipeline_active=False` again  \n",
    "\n",
    "Each step prints PASS/FAIL with the actual response so failures are easy to diagnose.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce80a55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request, json as _j, time\n",
    "\n",
    "BASE   = 'http://127.0.0.1:8000'\n",
    "SKIPHD = {'ngrok-skip-browser-warning': '1', 'Content-Type': 'application/json'}\n",
    "\n",
    "def _req(method, path, body=None, expect_code=200):\n",
    "    data_bytes = _j.dumps(body).encode() if body else None\n",
    "    req = urllib.request.Request(\n",
    "        f'{BASE}{path}',\n",
    "        data    = data_bytes,\n",
    "        headers = SKIPHD,\n",
    "        method  = method,\n",
    "    )\n",
    "    try:\n",
    "        with urllib.request.urlopen(req, timeout=30) as r:\n",
    "            code   = r.status\n",
    "            parsed = _j.loads(r.read())\n",
    "    except urllib.error.HTTPError as e:\n",
    "        code   = e.code\n",
    "        parsed = _j.loads(e.read() or b'{}')\n",
    "\n",
    "    ok  = (code == expect_code)\n",
    "    tag = 'PASS' if ok else 'FAIL'\n",
    "    print(f'  [{tag}]  {method} {path}  →  HTTP {code}')\n",
    "    if not ok:\n",
    "        print(f'         Expected {expect_code}, got {code}')\n",
    "        print(f'         Response: {_j.dumps(parsed, indent=6)[:400]}')\n",
    "    return code, parsed, ok\n",
    "\n",
    "passed = 0\n",
    "failed = 0\n",
    "\n",
    "# ── Test 1: Health ──────────────────────────────────────────────────────────\n",
    "code, data, ok = _req('GET', '/api/health', expect_code=200)\n",
    "assert ok and data.get('status') == 'ok'\n",
    "passed += 1\n",
    "print(f'         pipeline_active={data.get(\"pipeline_active\")}  surgery={data.get(\"surgery\")}')\n",
    "\n",
    "# ── Test 2: State without session → 400 ────────────────────────────────────\n",
    "code, data, ok = _req('GET', '/api/state', expect_code=400)\n",
    "assert ok and 'No active pipeline' in str(data.get('detail', ''))\n",
    "passed += 1\n",
    "\n",
    "# ── Test 3: Start session (heart transplant) ────────────────────────────────\n",
    "print()\n",
    "print('  Starting pipeline session for \"heart\" surgery...')\n",
    "print('  (MedGemma may already be loaded from Cell 9 — startup is fast)')\n",
    "code, data, ok = _req('POST', '/api/session/start', body={'surgery': 'heart'}, expect_code=200)\n",
    "assert ok and data.get('status') == 'ok'\n",
    "passed += 1\n",
    "print(f'         response: {_j.dumps(data)}')\n",
    "\n",
    "# ── Test 4: Health now shows pipeline active ────────────────────────────────\n",
    "time.sleep(2)   # give pipeline a moment to register\n",
    "code, data, ok = _req('GET', '/api/health', expect_code=200)\n",
    "assert ok and data.get('pipeline_active') is True\n",
    "passed += 1\n",
    "print(f'         pipeline_active={data[\"pipeline_active\"]}  surgery={data[\"surgery\"]}')\n",
    "\n",
    "# ── Test 5: State returns machine state dict ────────────────────────────────\n",
    "code, data, ok = _req('GET', '/api/state', expect_code=200)\n",
    "assert ok and data.get('status') == 'ok'\n",
    "states = data.get('state', {}).get('machine_states', {})\n",
    "assert isinstance(states, dict) and len(states) >= 1\n",
    "passed += 1\n",
    "print(f'         machine_states keys: {list(states.keys())}')\n",
    "\n",
    "# ── Test 6: WebSocket — connect and receive a state snapshot ────────────────\n",
    "print()\n",
    "print('  Connecting to WebSocket /ws/state...')\n",
    "try:\n",
    "    import websocket as _ws_lib   # websocket-client\n",
    "except ImportError:\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'websocket-client', '--quiet'], check=True)\n",
    "    import websocket as _ws_lib\n",
    "\n",
    "ws_messages = []\n",
    "\n",
    "def _on_message(ws, msg):\n",
    "    ws_messages.append(_j.loads(msg))\n",
    "    ws.close()\n",
    "\n",
    "def _on_error(ws, err):\n",
    "    print(f'  WebSocket error: {err}')\n",
    "\n",
    "_wsapp = _ws_lib.WebSocketApp(\n",
    "    'ws://127.0.0.1:8000/ws/state',\n",
    "    on_message = _on_message,\n",
    "    on_error   = _on_error,\n",
    ")\n",
    "import threading\n",
    "_wst = threading.Thread(target=_wsapp.run_forever, daemon=True)\n",
    "_wst.start()\n",
    "_wst.join(timeout=10)\n",
    "\n",
    "if ws_messages:\n",
    "    msg = ws_messages[0]\n",
    "    print(f'  [PASS]  WS /ws/state  →  received snapshot  surgery={msg.get(\"surgery\")}')\n",
    "    assert 'machine_states' in msg, 'Snapshot missing machine_states'\n",
    "    passed += 1\n",
    "else:\n",
    "    print('  [FAIL]  WS /ws/state  →  no message received within 10 s')\n",
    "    failed += 1\n",
    "\n",
    "# ── Test 7: Stop session ────────────────────────────────────────────────────\n",
    "print()\n",
    "code, data, ok = _req('POST', '/api/session/stop', expect_code=200)\n",
    "assert ok and data.get('status') == 'ok'\n",
    "passed += 1\n",
    "\n",
    "# ── Test 8: Health shows pipeline inactive ──────────────────────────────────\n",
    "time.sleep(2)\n",
    "code, data, ok = _req('GET', '/api/health', expect_code=200)\n",
    "assert ok and data.get('pipeline_active') is False\n",
    "passed += 1\n",
    "print(f'         pipeline_active={data[\"pipeline_active\"]} (should be False)')\n",
    "\n",
    "# ── Summary ─────────────────────────────────────────────────────────────────\n",
    "print()\n",
    "print('=' * 50)\n",
    "print(f'  E2E API Tests  — PASSED: {passed}  FAILED: {failed}')\n",
    "print('=' * 50)\n",
    "if failed > 0:\n",
    "    raise AssertionError(f'{failed} E2E test(s) FAILED — see output above.')\n",
    "\n",
    "print()\n",
    "print('Cell 14 PASSED ✓  — all E2E API tests green')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e115e615",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 15 — Local Frontend Testing  *(execute on your Windows PC, not Kaggle)*\n",
    "\n",
    "### Prerequisites check\n",
    "\n",
    "Open **PowerShell** on your local machine and run:\n",
    "```powershell\n",
    "node --version     # must be >= 18\n",
    "npm  --version     # must be >= 9\n",
    "```\n",
    "If missing → download from https://nodejs.org (LTS).\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1 — Install frontend dependencies (first time only)\n",
    "```powershell\n",
    "cd d:\\OR-SIM\\frontend\n",
    "npm install\n",
    "```\n",
    "Expected: `added NNN packages` with no errors.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2 — Set the backend URL in `.env.local`\n",
    "\n",
    "Copy the HTTPS URL printed by Cell 12 above, then run:\n",
    "```powershell\n",
    "# Replace the URL with YOUR ngrok URL from Cell 12 output:\n",
    "'VITE_BACKEND_URL=https://abcd-1234.ngrok-free.app' |\n",
    "    Out-File -Encoding utf8 d:\\OR-SIM\\frontend\\.env.local\n",
    "\n",
    "# Verify the file was written correctly:\n",
    "Get-Content d:\\OR-SIM\\frontend\\.env.local\n",
    "```\n",
    "Expected output:\n",
    "```\n",
    "VITE_BACKEND_URL=https://abcd-1234.ngrok-free.app\n",
    "```\n",
    "> **No trailing slash.** Copy the exact URL from Cell 12.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3 — Run a production build check (catches JSX/import errors)\n",
    "```powershell\n",
    "cd d:\\OR-SIM\\frontend\n",
    "npm run build\n",
    "```\n",
    "Expected output:\n",
    "```\n",
    "✓ NNN modules transformed.\n",
    "dist/index.html             x.xx kB\n",
    "dist/assets/index-XXXX.js   NNN.xx kB │ gzip:  NNN.xx kB\n",
    "✓ built in X.XXs\n",
    "```\n",
    "If the build fails, it prints the exact component and line number — fix before proceeding.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4 — Start the Vite dev server\n",
    "```powershell\n",
    "cd d:\\OR-SIM\\frontend\n",
    "npm run dev\n",
    "```\n",
    "Expected output:\n",
    "```\n",
    "  VITE v6.x.x  ready in ~641 ms\n",
    "\n",
    "  ➜  Local:   http://localhost:5173/\n",
    "  ➜  Network: use --host to expose\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Step 5 — Open the simulator and run an E2E session\n",
    "\n",
    "1. Open **http://localhost:5173** in Chrome or Edge\n",
    "2. The 3D OR room should render immediately (animated surgical lights, OR personnel, patient breathing)\n",
    "3. Open browser DevTools (F12) → Console — confirm **no red errors**\n",
    "4. Select a surgery from the dropdown (e.g. **Heart Transplant**)\n",
    "5. Click **▶ Start Session**\n",
    "   - The WebSocket status dot should turn **green** within 2 s\n",
    "   - The surgery name should appear in the top bar\n",
    "6. Allow microphone access when the browser asks\n",
    "7. Speak a command: *\"Activate the ventilator and patient monitor\"*\n",
    "   - The transcription bar at the bottom should display your words\n",
    "   - The matching machines in the 3D room should glow and animate\n",
    "8. Click **⏹ Stop Session** — machines go back to OFF state, dot turns grey\n",
    "\n",
    "---\n",
    "\n",
    "### Step 6 — Frontend checklist\n",
    "\n",
    "| Test | Expected result |\n",
    "|------|----------------|\n",
    "| Page loads at `localhost:5173` | 3D OR room renders, no white screen |\n",
    "| Console errors | **None** — DevTools console must be clean |\n",
    "| Surgery dropdown | All 20 surgeries listed; selecting one updates the machines |\n",
    "| Start Session | WS status dot turns green within 2 s |\n",
    "| Voice command | Transcription bar updates; machines change state |\n",
    "| Machine labels | All machine names clearly visible in the 3D scene |\n",
    "| Surgical lights ON | Large flat LED panels illuminate, visible cone effect |\n",
    "| Personnel animation | Surgeon/nurse arms move, patient chest rises/falls |\n",
    "| Stop Session | All machines OFF, session ends cleanly |\n",
    "| Changing surgery mid-session | Old pipeline stops, new one starts automatically |\n",
    "\n",
    "---\n",
    "\n",
    "### Troubleshooting\n",
    "\n",
    "| Symptom | Cause | Fix |\n",
    "|---------|-------|-----|\n",
    "| White screen / React error | JSX error | Run `npm run build` to see the exact line |\n",
    "| WS dot stays red | Wrong URL in `.env.local` | Re-run Step 2 with correct URL, restart `npm run dev` |\n",
    "| `ERR_NGROK_3200` | Free tier: 1 tunnel max | Close other ngrok sessions at dashboard.ngrok.com |\n",
    "| Microphone permission denied | Browser blocked mic | Click the lock icon in address bar → Allow microphone |\n",
    "| Transcription not updating | Audio not reaching server | Check console for WebSocket messages; verify mic works |\n",
    "| `npm run build` fails | Import or syntax error | Check the error line in the build output, fix the file |\n",
    "| `node_modules` missing | First run | `cd d:\\OR-SIM\\frontend && npm install` |\n",
    "| `.env.local` not picked up | Vite caches old env | Stop `npm run dev`, update file, restart |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc869da",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 16 — Keep-alive ping loop\n",
    "\n",
    "Prevents Kaggle from killing the kernel after ~30 min of inactivity.  \n",
    "**Stop with the Interrupt (■) button when you are done.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3f6e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime, time, urllib.request\n",
    "\n",
    "PING_INTERVAL = 60       # seconds between pings\n",
    "MAX_PINGS     = 720      # 720 × 60 s = 12 h (max Kaggle GPU session)\n",
    "\n",
    "SKIP_WARN = {'ngrok-skip-browser-warning': '1'}\n",
    "\n",
    "print(f'Keep-alive started — pinging every {PING_INTERVAL}s.')\n",
    "print('Stop with the Interrupt (■) button.')\n",
    "print()\n",
    "\n",
    "for _i in range(1, MAX_PINGS + 1):\n",
    "    time.sleep(PING_INTERVAL)\n",
    "\n",
    "    # Check server process is still alive\n",
    "    if _server_proc.poll() is not None:\n",
    "        print(f'[{datetime.datetime.now(datetime.timezone.utc).strftime(\"%H:%M:%S UTC\")}] '\n",
    "              f'Server process died (code {_server_proc.returncode}).')\n",
    "        print('Re-run Cells 11 and 12 to restart the server and tunnel.')\n",
    "        break\n",
    "\n",
    "    # Ping the local health endpoint (no ngrok overhead)\n",
    "    try:\n",
    "        with urllib.request.urlopen('http://127.0.0.1:8000/api/health', timeout=5) as _r:\n",
    "            _status = _r.status\n",
    "        _health_ok = True\n",
    "    except Exception:\n",
    "        _status    = 'error'\n",
    "        _health_ok = False\n",
    "\n",
    "    # Also ping through ngrok to keep the tunnel alive\n",
    "    try:\n",
    "        _req_ngrok = urllib.request.Request(\n",
    "            f'{NGROK_HTTPS}/api/health', headers=SKIP_WARN)\n",
    "        with urllib.request.urlopen(_req_ngrok, timeout=8) as _r_ngrok:\n",
    "            _ngrok_status = _r_ngrok.status\n",
    "    except Exception:\n",
    "        _ngrok_status = 'error'\n",
    "\n",
    "    ts = datetime.datetime.now(datetime.timezone.utc).strftime('%H:%M:%S UTC')\n",
    "    print(f'  [{ts}]  ping #{_i:04d}  local→{_status}  ngrok→{_ngrok_status}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
