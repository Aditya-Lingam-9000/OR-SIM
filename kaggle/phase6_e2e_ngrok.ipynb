{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43bf7b94",
   "metadata": {},
   "source": [
    "# OR-SIM — End-to-End: Kaggle Backend + Local Frontend via ngrok\n",
    "\n",
    "**Environment**: Kaggle GPU T4x2 or P100 (>=15 GB VRAM)\n",
    "\n",
    "```\n",
    "Your Machine                          Kaggle Notebook (GPU)\n",
    "\n",
    "Browser  http://localhost:5173       |  uvicorn :8000  (FastAPI + MedGemma + MedASR)\n",
    "   Vite dev-server                   |       ↑\n",
    "          ↕ HTTPS / WSS              |  ngrok tunnel\n",
    "   ngrok public URL ─────────────────┘\n",
    "```\n",
    "\n",
    "## Steps at a glance\n",
    "| # | Cell | Action |\n",
    "|---|------|--------|\n",
    "| 1 | GPU check | Verify T4/P100 is present |\n",
    "| 2 | Clone repo | Pull OR-SIM from GitHub |\n",
    "| 3 | llama-cpp | CUDA 12.4 prebuilt wheel (fast, no compile) |\n",
    "| 4 | Python deps | Install requirements/base + asr + server |\n",
    "| 5 | MedGemma | Download GGUF from `unsloth/medgemma-4b-it-GGUF` (~2.1 GB) |\n",
    "| 6 | MedASR | Download int8 ONNX baseline (Part A, always runs); optionally export google/medasr (Part B) |\n",
    "| 7 | ngrok | Install pyngrok + set auth token |\n",
    "| 8 | Server | Launch FastAPI backend (subprocess) |\n",
    "| 9 | Tunnel | Open ngrok — **copy the HTTPS URL** |\n",
    "| 10 | Frontend | Step-by-step: set .env.local and start Vite |\n",
    "| 11 | Sanity | Confirm tunnel reaches /api/health and /api/state |\n",
    "| 12 | Keep-alive | Ping loop — prevents Kaggle idle timeout |\n",
    "\n",
    "## Prerequisites\n",
    "| Requirement | Where |\n",
    "|---|---|\n",
    "| Kaggle GPU notebook (T4 x2 or P100) | Notebook Settings → Accelerator |\n",
    "| Internet access ON | Notebook Settings → Internet |\n",
    "| ngrok auth token | https://dashboard.ngrok.com/get-started/your-authtoken |\n",
    "| OR-SIM pushed to GitHub | Already done — all phases committed |\n",
    "\n",
    "## Only one thing required before running\n",
    "| Variable | Cell | What to set |\n",
    "|---|---|---|\n",
    "| `NGROK_AUTH_TOKEN` | 7 | Your ngrok auth token (or use a Kaggle Secret) |\n",
    "\n",
    "> **HF token is optional** — Cell 6 Part A downloads a public int8 ASR model (no token needed).  \n",
    "> To upgrade to the higher-quality `google/medasr` (6.6% WER), also complete Cell 6 Part B.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5ee596",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 1 — Verify GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df01c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, sys\n",
    "\n",
    "result = subprocess.run(\n",
    "    ['nvidia-smi', '--query-gpu=name,memory.total,driver_version', '--format=csv,noheader'],\n",
    "    capture_output=True, text=True,\n",
    ")\n",
    "if result.returncode == 0:\n",
    "    print('GPU:', result.stdout.strip())\n",
    "else:\n",
    "    print('No GPU detected — go to Notebook Settings > Accelerator and enable GPU.')\n",
    "    sys.exit('GPU required')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189fc787",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 2 — Clone OR-SIM repository\n",
    "\n",
    "> **`REPO_URL` is pre-filled with your GitHub repo.  No changes needed.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ef2dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "REPO_URL = 'https://github.com/Aditya-Lingam-9000/OR-SIM.git'\n",
    "REPO_DIR = '/kaggle/working/OR-SIM'\n",
    "\n",
    "if not os.path.isdir(os.path.join(REPO_DIR, '.git')):\n",
    "    subprocess.run(['git', 'clone', '--depth', '1', REPO_URL, REPO_DIR], check=True)\n",
    "    print(f'Cloned -> {REPO_DIR}')\n",
    "else:\n",
    "    subprocess.run(['git', '-C', REPO_DIR, 'pull', '--ff-only'], check=True)\n",
    "    print(f'Updated existing clone at {REPO_DIR}')\n",
    "\n",
    "# Add repo root to sys.path so backend.* packages are importable in all later cells\n",
    "if REPO_DIR not in sys.path:\n",
    "    sys.path.insert(0, REPO_DIR)\n",
    "\n",
    "# Show the 5 most recent commits as a sanity check\n",
    "subprocess.run(['git', '-C', REPO_DIR, 'log', '--oneline', '-5'], check=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf62ce36",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 3 — Install llama-cpp-python (CUDA 12.4 prebuilt wheel — no compilation)\n",
    "\n",
    "Kaggle T4 instances ship CUDA 12.4.  \n",
    "We use the prebuilt wheel from `abetlen.github.io` — identical to the Phase 3 approach.\n",
    "Takes ~30 s vs ~5 minutes if built from source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cd4dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.run(\n",
    "    [\n",
    "        sys.executable, '-m', 'pip', 'install',\n",
    "        'llama-cpp-python',\n",
    "        '--extra-index-url', 'https://abetlen.github.io/llama-cpp-python/whl/cu124',\n",
    "        '--quiet',\n",
    "    ],\n",
    "    check=True,\n",
    ")\n",
    "\n",
    "# Verify GPU offload was compiled in — MUST be True for decent latency\n",
    "import llama_cpp\n",
    "lib    = getattr(llama_cpp, 'llama_cpp', None)\n",
    "gpu_ok = lib.llama_supports_gpu_offload() if lib else False\n",
    "print(f'llama_supports_gpu_offload() = {gpu_ok}')\n",
    "if not gpu_ok:\n",
    "    raise RuntimeError(\n",
    "        'GPU offload not compiled in.  '\n",
    "        'Check nvidia-smi shows CUDA 12.x — the cu124 wheel requires CUDA >= 12.4.'\n",
    "    )\n",
    "\n",
    "from llama_cpp import Llama\n",
    "print('llama-cpp-python imported successfully')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6881396f",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 4 — Install OR-SIM Python dependencies\n",
    "\n",
    "Installs from the repo's own requirements files (`base.txt`, `asr.txt`, `server.txt`).  \n",
    "`requirements/llm.txt` is **intentionally skipped** — `llama-cpp-python` was already installed\n",
    "from the CUDA 12.4 prebuilt wheel in Cell 3; running pip against `llm.txt` would replace it\n",
    "with the slower CPU-only build from PyPI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da9af56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PortAudio C library — required by sounddevice (used in requirements/asr.txt).\n",
    "# sounddevice is imported at module level inside backend/asr, so it must be\n",
    "# importable even though there is no real microphone on Kaggle.\n",
    "subprocess.run(\n",
    "    ['apt-get', 'install', '-y', '-q', 'libportaudio2'],\n",
    "    check=False,   # non-fatal — package may already be present on some Kaggle images\n",
    ")\n",
    "\n",
    "# Install from the repo's requirements files.\n",
    "# llm.txt is intentionally excluded — llama-cpp-python was installed with CUDA\n",
    "# support in Cell 3; installing it again from PyPI would overwrite the CUDA wheel.\n",
    "for req_file in ('base.txt', 'asr.txt', 'server.txt'):\n",
    "    subprocess.run(\n",
    "        [\n",
    "            sys.executable, '-m', 'pip', 'install', '--quiet',\n",
    "            '-r', os.path.join(REPO_DIR, 'requirements', req_file),\n",
    "        ],\n",
    "        check=True,\n",
    "    )\n",
    "    print(f'  requirements/{req_file} installed')\n",
    "\n",
    "# huggingface_hub is used in Cells 5 and 6 for model downloads\n",
    "subprocess.run(\n",
    "    [sys.executable, '-m', 'pip', 'install', '--quiet', 'huggingface_hub'],\n",
    "    check=True,\n",
    ")\n",
    "print('huggingface_hub installed')\n",
    "print('All OR-SIM Python dependencies installed')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26ddae3",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 5 — Download MedGemma GGUF from HuggingFace\n",
    "\n",
    "Model: `unsloth/medgemma-4b-it-GGUF`  `medgemma-4b-it-Q3_K_M.gguf` (~2.1 GB)  \n",
    "Downloaded directly into the cloned repo at the path the backend expects.  \n",
    "**No dataset upload required** — identical to the Phase 3 approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb94a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "from pathlib import Path\n",
    "\n",
    "GGUF_DIR = Path(REPO_DIR) / 'models' / 'medgemma'\n",
    "GGUF_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "EXPECTED_GGUF = GGUF_DIR / 'medgemma-4b-it-Q3_K_M.gguf'\n",
    "\n",
    "# Skip download if already present (e.g. re-running the notebook)\n",
    "if EXPECTED_GGUF.exists():\n",
    "    MODEL_PATH = EXPECTED_GGUF\n",
    "    print(f'GGUF already present ({MODEL_PATH.stat().st_size / 1e9:.2f} GB) — skipping download.')\n",
    "else:\n",
    "    print('Downloading medgemma-4b-it-Q3_K_M.gguf from HuggingFace...')\n",
    "    print('(~2.1 GB — takes 2-5 minutes on Kaggle)')\n",
    "    MODEL_PATH = Path(hf_hub_download(\n",
    "        repo_id   = 'unsloth/medgemma-4b-it-GGUF',\n",
    "        filename  = 'medgemma-4b-it-Q3_K_M.gguf',\n",
    "        local_dir = str(GGUF_DIR),\n",
    "    ))\n",
    "\n",
    "print(f'Model path : {MODEL_PATH}')\n",
    "print(f'File size  : {MODEL_PATH.stat().st_size / 1e9:.2f} GB')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdf584a",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 6 — Download MedASR models\n",
    "\n",
    "**Part A — Tier 2 baseline (always runs, ~3 min, no token needed)**  \n",
    "Downloads `csukuangfj/sherpa-onnx-medasr-ctc-en-int8-2025-12-25` (public repo).  \n",
    "This ensures the server never crashes at startup regardless of HF token status.\n",
    "\n",
    "**Part B — Tier 1 upgrade (optional, ~5 min, requires HF token)**  \n",
    "Downloads `google/medasr` PyTorch weights directly — **no ONNX conversion**, no export errors.  \n",
    "Before running: accept terms at [huggingface.co/google/medasr](https://huggingface.co/google/medasr)  \n",
    "and set `HF_TOKEN` below or add a Kaggle Secret named `HF_TOKEN`.\n",
    "\n",
    "Active model priority at runtime:\n",
    "| Tier | Model | WER | Requires |\n",
    "|------|-------|-----|---------|\n",
    "| 1 | google/medasr PyTorch (local) | 6.6% | Cell 6 Part B + HF token |\n",
    "| 2 | sherpa-onnx int8 | ~18% | Cell 6 Part A only |\n",
    "| 3 | google/medasr PyTorch (online) | 6.6% | HUGGING_FACE_HUB_TOKEN env |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2891790",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "ASR_DIR = Path(REPO_DIR) / 'models' / 'medasr'\n",
    "ASR_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# PART A — Tier 2 baseline: sherpa-onnx int8 ONNX (public, no token needed)\n",
    "# Always runs. This guarantees the backend can start even without a HF token.\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "INT8_REPO = 'csukuangfj/sherpa-onnx-medasr-ctc-en-int8-2025-12-25'\n",
    "for fname in ('model.int8.onnx', 'tokens.txt'):\n",
    "    fpath = ASR_DIR / fname\n",
    "    if fpath.exists():\n",
    "        print(f'  [Tier 2] {fname} already present ({fpath.stat().st_size / 1e6:.1f} MB) — skipping.')\n",
    "    else:\n",
    "        print(f'  [Tier 2] Downloading {fname} from {INT8_REPO} ...')\n",
    "        hf_hub_download(repo_id=INT8_REPO, filename=fname, local_dir=str(ASR_DIR))\n",
    "        print(f'  [Tier 2] Downloaded {fname} ({(ASR_DIR / fname).stat().st_size / 1e6:.1f} MB)')\n",
    "\n",
    "assert (ASR_DIR / 'model.int8.onnx').exists(), 'int8 ONNX download failed!'\n",
    "print()\n",
    "print('Tier 2 (int8 ONNX) ready — server will start using this model.')\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# PART B — Tier 1 upgrade: download google/medasr PyTorch weights (no conversion)\n",
    "# Skip this block if you don't have a token — Tier 2 above is enough.\n",
    "# Note: no ONNX export — the backend loads the PyTorch model directly.\n",
    "#       This avoids all LasrFeatureExtractor export errors.\n",
    "# ═══════════════════════════════════════════════════════════════════════════════\n",
    "# Set your HuggingFace token here (or add a Kaggle Secret named HF_TOKEN):\n",
    "HF_TOKEN = ''   # ← paste: hf_xxxxxxxxxxxxxxxxxxxx\n",
    "\n",
    "# Try Kaggle Secret if empty\n",
    "if not HF_TOKEN:\n",
    "    try:\n",
    "        from kaggle_secrets import UserSecretsClient\n",
    "        HF_TOKEN = UserSecretsClient().get_secret('HF_TOKEN')\n",
    "        print('HF_TOKEN loaded from Kaggle Secret')\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "if not HF_TOKEN:\n",
    "    print()\n",
    "    print('No HF_TOKEN set — skipping Tier 1 (google/medasr PyTorch) download.')\n",
    "    print('The server will use Tier 2 (int8 ONNX). This is fine for a demo.')\n",
    "    print('To upgrade: accept terms at https://huggingface.co/google/medasr')\n",
    "    print('            then set HF_TOKEN above and re-run this cell.')\n",
    "else:\n",
    "    TORCH_DIR = ASR_DIR / 'pytorch'\n",
    "\n",
    "    # Check if already downloaded (config.json is always present in a valid snapshot)\n",
    "    if (TORCH_DIR / 'config.json').exists():\n",
    "        size_mb = sum(f.stat().st_size for f in TORCH_DIR.rglob('*') if f.is_file()) / 1e6\n",
    "        print(f'[Tier 1] google/medasr already downloaded ({size_mb:.0f} MB) — skipping.')\n",
    "    else:\n",
    "        TORCH_DIR.mkdir(parents=True, exist_ok=True)\n",
    "        print('[Tier 1] Downloading google/medasr PyTorch weights (~420 MB)...')\n",
    "        print('         (no ONNX conversion — loading model directly at runtime)')\n",
    "        try:\n",
    "            from huggingface_hub import snapshot_download\n",
    "            snapshot_download(\n",
    "                repo_id   = 'google/medasr',\n",
    "                local_dir = str(TORCH_DIR),\n",
    "                token     = HF_TOKEN,\n",
    "                ignore_patterns=['*.msgpack', '*.h5', 'flax_model*', 'tf_model*', 'rust_model*'],\n",
    "            )\n",
    "            size_mb = sum(f.stat().st_size for f in TORCH_DIR.rglob('*') if f.is_file()) / 1e6\n",
    "            print(f'[Tier 1] google/medasr downloaded ({size_mb:.0f} MB) ✓')\n",
    "            print('[Tier 1] Server will use Tier 1 (google/medasr PyTorch, 6.6% WER).')\n",
    "        except Exception as e:\n",
    "            print(f'[Tier 1] Download failed ({type(e).__name__}: {e})')\n",
    "            print('[Tier 1] Server will fall back to Tier 2 (int8 ONNX). No action needed.')\n",
    "\n",
    "print()\n",
    "print('Cell 6 complete.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d9fdd6",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 7 — Install pyngrok and configure your auth token\n",
    "\n",
    "1. Sign up (free) at https://ngrok.com  \n",
    "2. Copy your token from https://dashboard.ngrok.com/get-started/your-authtoken  \n",
    "3. Either paste it directly OR store it as a Kaggle Secret (recommended):\n",
    "   - *Notebook Settings  Add-ons  Secrets  Add* — key: `NGROK_TOKEN`\n",
    "   - Uncomment the `UserSecretsClient` lines and remove the placeholder string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecdb66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.run(\n",
    "    [sys.executable, '-m', 'pip', 'install', 'pyngrok', '--quiet'],\n",
    "    check=True,\n",
    ")\n",
    "from pyngrok import ngrok\n",
    "print('pyngrok installed')\n",
    "\n",
    "#  CONFIGURE \n",
    "# Option A — paste token directly (do NOT commit a notebook with a real token)\n",
    "NGROK_AUTH_TOKEN = 'YOUR_NGROK_AUTH_TOKEN'\n",
    "\n",
    "# Option B — Kaggle Secret (recommended for shared notebooks)\n",
    "# from kaggle_secrets import UserSecretsClient\n",
    "# NGROK_AUTH_TOKEN = UserSecretsClient().get_secret('NGROK_TOKEN')\n",
    "# \n",
    "\n",
    "if NGROK_AUTH_TOKEN == 'YOUR_NGROK_AUTH_TOKEN':\n",
    "    raise ValueError(\n",
    "        'Paste your real ngrok token above or use a Kaggle Secret.'\n",
    "        '  Get it from https://dashboard.ngrok.com/get-started/your-authtoken'\n",
    "    )\n",
    "\n",
    "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
    "print('ngrok auth token configured')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70170fd5",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 8 — Start the FastAPI backend server\n",
    "\n",
    "The server is launched as a **subprocess** (`python -m backend.server`).\n",
    "Using a subprocess (not a thread) avoids asyncio event-loop conflicts between uvicorn\n",
    "and Jupyter's own event loop — the most common cause of silent startup failures on Kaggle.\n",
    "\n",
    "MedGemma loads into GPU VRAM during startup — allow up to **60 seconds**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51a073d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket, time, urllib.request, json as _json\n",
    "\n",
    "# Stop any server left from a previous run of this cell\n",
    "try:\n",
    "    if _server_proc.poll() is None:\n",
    "        _server_proc.terminate()\n",
    "        _server_proc.wait(timeout=5)\n",
    "        print('Stopped previous server instance.')\n",
    "except NameError:\n",
    "    pass  # first run\n",
    "\n",
    "env = os.environ.copy()\n",
    "env['PYTHONPATH'] = REPO_DIR\n",
    "\n",
    "# python -m backend.server  is the canonical entry-point (same as local dev)\n",
    "_server_proc = subprocess.Popen(\n",
    "    [sys.executable, '-m', 'backend.server', '--host', '0.0.0.0', '--port', '8000'],\n",
    "    cwd=REPO_DIR,\n",
    "    env=env,\n",
    "    # stdout/stderr flow to the Kaggle cell output so startup logs are visible\n",
    ")\n",
    "print(f'Server PID: {_server_proc.pid}')\n",
    "print('Waiting for FastAPI to bind port 8000', end='', flush=True)\n",
    "\n",
    "deadline   = time.time() + 60\n",
    "connected  = False\n",
    "while time.time() < deadline:\n",
    "    if _server_proc.poll() is not None:\n",
    "        raise RuntimeError(\n",
    "            f'Server process exited early (exit code {_server_proc.returncode}).\\n'\n",
    "            'Scroll up for the traceback.'\n",
    "        )\n",
    "    try:\n",
    "        with socket.create_connection(('127.0.0.1', 8000), timeout=1):\n",
    "            connected = True\n",
    "            break\n",
    "    except OSError:\n",
    "        print('.', end='', flush=True)\n",
    "        time.sleep(1)\n",
    "\n",
    "print()\n",
    "if not connected:\n",
    "    raise TimeoutError('Server did not bind within 60 s — check the logs above.')\n",
    "\n",
    "# Quick health-check via plain urllib (no extra libraries needed)\n",
    "try:\n",
    "    with urllib.request.urlopen('http://127.0.0.1:8000/api/health', timeout=5) as r:\n",
    "        health = _json.loads(r.read())\n",
    "    print(f'Server healthy: {health}')\n",
    "except Exception as e:\n",
    "    print(f'Health check failed: {e}  — server may still be loading MedGemma, re-run this cell.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3ea3e6",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 9 — Open ngrok tunnel and print the public URLs\n",
    "\n",
    "**Copy the HTTPS URL printed below — you will paste it into `frontend/.env.local`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fe2547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close any stale tunnels from previous runs\n",
    "for _t in ngrok.get_tunnels():\n",
    "    ngrok.disconnect(_t.public_url)\n",
    "\n",
    "tunnel      = ngrok.connect(8000, bind_tls=True)\n",
    "NGROK_HTTPS = tunnel.public_url\n",
    "NGROK_WSS   = NGROK_HTTPS.replace('https://', 'wss://')\n",
    "\n",
    "print()\n",
    "print('=' * 65)\n",
    "print('  OR-SIM BACKEND IS LIVE')\n",
    "print('=' * 65)\n",
    "print(f'  HTTPS (REST API) : {NGROK_HTTPS}')\n",
    "print(f'  WSS  (WebSocket) : {NGROK_WSS}/ws/state')\n",
    "print('=' * 65)\n",
    "print()\n",
    "print('Copy the HTTPS URL, then follow Cell 10 on your local machine.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910cad93",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 10 — Connect the local frontend  *(run on YOUR machine, not Kaggle)*\n",
    "\n",
    "### Step 1 — Create `frontend/.env.local`\n",
    "\n",
    "```\n",
    "# d:\\OR-SIM\\frontend\\.env.local\n",
    "VITE_BACKEND_URL=https://abcd-1234.ngrok-free.app\n",
    "```\n",
    "\n",
    "Replace `https://abcd-1234.ngrok-free.app` with the **HTTPS URL from Cell 9**. No trailing slash.\n",
    "\n",
    "PowerShell one-liner (edit the URL first):\n",
    "```powershell\n",
    "'VITE_BACKEND_URL=https://abcd-1234.ngrok-free.app' | Out-File -Encoding utf8 d:\\OR-SIM\\frontend\\.env.local\n",
    "```\n",
    "\n",
    "### Step 2 — Start the Vite dev server\n",
    "\n",
    "```powershell\n",
    "cd d:\\OR-SIM\\frontend\n",
    "npm run dev\n",
    "```\n",
    "\n",
    "Expected output:\n",
    "```\n",
    "  VITE v6.x.x  ready in ~641 ms\n",
    "  -> Local:   http://localhost:5173/\n",
    "```\n",
    "\n",
    "### Step 3 — Open the simulator\n",
    "\n",
    "1. Open **http://localhost:5173** in Chrome or Edge.\n",
    "2. Pick a surgery (Heart Transplant / Liver Resection / Kidney PCNL).\n",
    "3. Click **Start Session**.\n",
    "4. Allow microphone access when the browser asks.\n",
    "5. Speak surgical commands — machines glow ON/OFF in the 3D room in real-time!\n",
    "\n",
    "### Stopping\n",
    "\n",
    "- Click **Stop Session** in the browser.\n",
    "- Stop the Kaggle kernel (square button) to free GPU VRAM.\n",
    "- The ngrok tunnel closes automatically with the kernel.\n",
    "\n",
    "### Troubleshooting\n",
    "\n",
    "| Symptom | Fix |\n",
    "|---|---|\n",
    "| WS status dot stays red | Check `.env.local` — no trailing slash, correct URL |\n",
    "| `ERR_NGROK_3200` | Free tier allows 1 tunnel; close other open ngrok sessions |\n",
    "| `ModuleNotFoundError: backend` | Re-run Cell 2 (sys.path), then Cell 8 |\n",
    "| `sounddevice` ImportError | Re-run Cell 4 (installs libportaudio2) |\n",
    "| Server exits immediately | Scroll up in Cell 8 for the Python traceback |\n",
    "| GGUF not found | Check Cell 5 printed a valid file size |\n",
    "| Kaggle kernel idle timeout | Re-run Cells 8 and 9 to restart server and tunnel |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c55cef",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 11 — Sanity check: confirm tunnel reaches the API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0837e299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ngrok adds a browser-warning page for unauthenticated requests.\n",
    "# The header below skips it so we get the raw JSON response.\n",
    "SKIP_WARN = {'ngrok-skip-browser-warning': '1'}\n",
    "\n",
    "for path, label in [('/api/health', 'Health'), ('/api/state', 'State')]:\n",
    "    url = f'{NGROK_HTTPS}{path}'\n",
    "    try:\n",
    "        req = urllib.request.Request(url, headers=SKIP_WARN)\n",
    "        with urllib.request.urlopen(req, timeout=10) as r:\n",
    "            data = _json.loads(r.read())\n",
    "        preview = _json.dumps(data, indent=2)[:400]\n",
    "        print(f'{label}:\\n{preview}\\n')\n",
    "    except Exception as e:\n",
    "        print(f'{label} check failed: {e}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6169a752",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 12 — Keep-alive ping *(optional — run while using the simulator)*\n",
    "\n",
    "Kaggle kills idle kernels after ~30 minutes.  Interrupt the cell (square button) to stop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8007636",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "PING_INTERVAL = 60     # seconds between pings\n",
    "MAX_PINGS     = 720    # 720 x 60 s = 12 hours (full Kaggle session limit)\n",
    "\n",
    "print(f'Keep-alive running — ping every {PING_INTERVAL}s.  Stop with the interrupt button.')\n",
    "for _i in range(1, MAX_PINGS + 1):\n",
    "    time.sleep(PING_INTERVAL)\n",
    "\n",
    "    if _server_proc.poll() is not None:\n",
    "        print(f'Server process died (code {_server_proc.returncode}).  Re-run Cell 8 + 9.')\n",
    "        break\n",
    "\n",
    "    try:\n",
    "        _req = urllib.request.Request(f'{NGROK_HTTPS}/api/health', headers=SKIP_WARN)\n",
    "        with urllib.request.urlopen(_req, timeout=5) as _r:\n",
    "            _status = _r.status\n",
    "    except Exception:\n",
    "        _status = 'error'\n",
    "\n",
    "    _ts = datetime.datetime.now(datetime.timezone.utc).strftime('%H:%M:%S UTC')\n",
    "    print(f'  [{_ts}] ping #{_i:03d} -> HTTP {_status}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a95643",
   "metadata": {},
   "source": [
    "---\n",
    "# Complete Step-by-Step Run Instructions\n",
    "\n",
    "## Part A — Kaggle (backend)\n",
    "\n",
    "### Step 1 — Create a new Kaggle notebook\n",
    "1. Go to https://www.kaggle.com → **New Notebook**\n",
    "2. Click **File → Import Notebook** → upload `kaggle/phase6_e2e_ngrok.ipynb`  \n",
    "   **OR** open the notebook directly from the cloned repo in your workspace\n",
    "3. **Notebook Settings** (gear icon) → set:\n",
    "   - Accelerator: **GPU T4 x2** (or P100)\n",
    "   - Internet: **On**\n",
    "\n",
    "### Step 2 — Configure ngrok token (Cell 7)\n",
    "1. Sign up free at https://ngrok.com\n",
    "2. Get your token from https://dashboard.ngrok.com/get-started/your-authtoken\n",
    "3. In Cell 7, either:\n",
    "   - Paste directly: `NGROK_AUTH_TOKEN = 'your_real_token_here'`\n",
    "   - **Recommended** — Kaggle Secret:\n",
    "     - Notebook sidebar → **Add-ons → Secrets → + Add**\n",
    "     - Key: `NGROK_TOKEN`, Value: your token\n",
    "     - Uncomment the two `UserSecretsClient` lines in Cell 7\n",
    "\n",
    "### Step 3 — Run all cells in order (1 → 12)\n",
    "| Cell | Expected output | Approx time |\n",
    "|------|----------------|-------------|\n",
    "| 1 | `GPU: Tesla T4, 15360 MiB, 525.xx` | instant |\n",
    "| 2 | `Cloned -> /kaggle/working/OR-SIM` | ~5 s |\n",
    "| 3 | `llama_supports_gpu_offload() = True` | ~30 s |\n",
    "| 4 | `All OR-SIM Python dependencies installed` | ~60 s |\n",
    "| 5 | `File size  : 2.1x GB` | ~3-5 min |\n",
    "| 6 | `MedASR model files ready` | ~30 s |\n",
    "| 7 | `ngrok auth token configured` | instant |\n",
    "| 8 | `Server healthy: {'status': 'ok', ...}` | ~30-60 s |\n",
    "| **9** | **`HTTPS (REST API) : https://xxxx.ngrok-free.app`** | instant |\n",
    "| 11 | `Health: {...}` and `State: {...}` | ~5 s |\n",
    "| 12 | `ping #001 -> HTTP 200` (keep running) | ongoing |\n",
    "\n",
    "### Step 4 — Copy the ngrok URL\n",
    "When Cell 9 prints:\n",
    "```\n",
    "=================================================================\n",
    "  OR-SIM BACKEND IS LIVE\n",
    "=================================================================\n",
    "  HTTPS (REST API) : https://abcd-1234.ngrok-free.app\n",
    "  WSS  (WebSocket) : wss://abcd-1234.ngrok-free.app/ws/state\n",
    "=================================================================\n",
    "```\n",
    "**Copy the HTTPS URL** (e.g. `https://abcd-1234.ngrok-free.app`).\n",
    "\n",
    "---\n",
    "\n",
    "## Part B — Local Machine (frontend)\n",
    "\n",
    "### Step 5 — Create `frontend/.env.local`\n",
    "\n",
    "Open a new PowerShell window and run (replace the URL with yours):\n",
    "\n",
    "```powershell\n",
    "'VITE_BACKEND_URL=https://abcd-1234.ngrok-free.app' |\n",
    "    Out-File -Encoding utf8 d:\\OR-SIM\\frontend\\.env.local\n",
    "```\n",
    "\n",
    "Or open `d:\\OR-SIM\\frontend\\.env.local` in any text editor and type:\n",
    "```\n",
    "VITE_BACKEND_URL=https://abcd-1234.ngrok-free.app\n",
    "```\n",
    "> **No trailing slash.** Copy the exact URL from Cell 9 output.\n",
    "\n",
    "### Step 6 — Start the Vite dev server\n",
    "\n",
    "```powershell\n",
    "cd d:\\OR-SIM\\frontend\n",
    "npm run dev\n",
    "```\n",
    "\n",
    "Expected output:\n",
    "```\n",
    "  VITE v6.x.x  ready in 641 ms\n",
    "\n",
    "  ➜  Local:   http://localhost:5173/\n",
    "  ➜  Network: use --host to expose\n",
    "```\n",
    "\n",
    "### Step 7 — Open the simulator\n",
    "\n",
    "1. Open **http://localhost:5173** in Chrome or Edge\n",
    "2. Select a surgery:  `Heart Transplant` / `Liver Resection` / `Kidney PCNL`\n",
    "3. Click **▶ Start Session** — the WS status dot should turn **green**\n",
    "4. Allow microphone access when the browser asks\n",
    "5. Speak surgical commands — machines glow ON in the 3D room in real time!\n",
    "\n",
    "---\n",
    "\n",
    "## Part C — Updating the ngrok URL (each new Kaggle session)\n",
    "\n",
    "Every time you restart the Kaggle kernel, ngrok generates a **new URL**.\n",
    "\n",
    "```powershell\n",
    "# On your local machine — update the URL and restart Vite:\n",
    "'VITE_BACKEND_URL=https://NEW-URL.ngrok-free.app' |\n",
    "    Out-File -Encoding utf8 d:\\OR-SIM\\frontend\\.env.local\n",
    "\n",
    "# Ctrl+C to stop the current Vite session, then:\n",
    "cd d:\\OR-SIM\\frontend\n",
    "npm run dev\n",
    "```\n",
    "\n",
    "You do **not** need to rebuild (`npm run build`) — Vite hot-reloads `.env.local` on restart.\n",
    "\n",
    "---\n",
    "\n",
    "## Part D — Stopping cleanly\n",
    "\n",
    "1. Click **⏹ Stop Session** in the browser UI\n",
    "2. Interrupt Cell 12 (the ping loop) with the **square stop button** in Kaggle\n",
    "3. Kill the Kaggle kernel to free GPU VRAM (≈15 GB)\n",
    "4. The ngrok tunnel closes automatically when the kernel dies\n",
    "\n",
    "---\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "| Symptom | Cause | Fix |\n",
    "|---------|-------|-----|\n",
    "| WS dot stays red/yellow | Wrong URL in `.env.local` | Check no trailing slash; re-copy from Cell 9 |\n",
    "| `ERR_NGROK_3200` | Free tier: only 1 tunnel allowed | Close other ngrok sessions; check dashboard |\n",
    "| `GPU offload not compiled in` | Wrong CUDA version wheel | Re-run Cell 3; Cell 1 confirms `525.xx` driver → cu124 is correct |\n",
    "| `ModuleNotFoundError: backend` | sys.path not set | Re-run Cell 2 (sets sys.path), then Cell 8 |\n",
    "| Cell 8 exits early | MedGemma not loaded | Check Cell 5 shows 2.1 GB; re-run Cell 8 |\n",
    "| `sounddevice` error in Cell 4 | libportaudio2 missing | Cell 4 installs it; if it fails run: `!apt-get install -y libportaudio2` manually |\n",
    "| Session start fails in browser | Server can't find model | Check Cell 5 and 6 both printed success |\n",
    "| Kaggle timeout after ~30 min | Idle kernel | Cell 12 prevents this; if it stopped, re-run Cells 8 + 9 + 12 |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
