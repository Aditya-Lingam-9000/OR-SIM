{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43bf7b94",
   "metadata": {},
   "source": [
    "# OR-SIM — End-to-End: Kaggle Backend + Local Frontend via ngrok\n",
    "\n",
    "**Environment**: Kaggle GPU T4x2 or P100 (>=15 GB VRAM)\n",
    "\n",
    "```\n",
    "Your Machine                          Kaggle Notebook (GPU)\n",
    "       \n",
    "Browser  http://localhost:5173       |  uvicorn :8000  (FastAPI + MedGemma)\n",
    "   Vite dev-server                    |  MedASR ONNX    (CPU/GPU)\n",
    "          HTTPS / WSS               |        \n",
    "   ngrok public URL +  ngrok tunnel\n",
    "```\n",
    "\n",
    "## Steps at a glance\n",
    "| # | Cell | Action |\n",
    "|---|------|--------|\n",
    "| 1 | GPU check | Verify T4/P100 is present |\n",
    "| 2 | Clone repo | Pull OR-SIM from GitHub |\n",
    "| 3 | llama-cpp | CUDA 12.4 prebuilt wheel (fast, no compile) |\n",
    "| 4 | Python deps | FastAPI, ONNX, pydantic, sounddevice, etc. |\n",
    "| 5 | MedGemma | Download GGUF from HuggingFace (~2.1 GB) |\n",
    "| 6 | MedASR | Verify ONNX model files are in the repo |\n",
    "| 7 | ngrok | Install pyngrok + set auth token |\n",
    "| 8 | Server | Launch FastAPI backend (subprocess) |\n",
    "| 9 | Tunnel | Open ngrok — **copy the HTTPS URL** |\n",
    "| 10 | Frontend | Instructions for your local machine (markdown) |\n",
    "| 11 | Sanity | Confirm tunnel reaches /api/health and /api/state |\n",
    "| 12 | Keep-alive | Ping loop — prevents Kaggle idle timeout |\n",
    "\n",
    "## Prerequisites\n",
    "| Requirement | Where |\n",
    "|---|---|\n",
    "| Kaggle GPU notebook (T4 x2 or P100) | Notebook Settings  Accelerator |\n",
    "| Internet access ON | Notebook Settings  Internet |\n",
    "| ngrok auth token | https://dashboard.ngrok.com/get-started/your-authtoken |\n",
    "| OR-SIM pushed to GitHub | Already done — all phases committed |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5ee596",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 1 — Verify GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df01c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, sys\n",
    "\n",
    "result = subprocess.run(\n",
    "    ['nvidia-smi', '--query-gpu=name,memory.total,driver_version', '--format=csv,noheader'],\n",
    "    capture_output=True, text=True,\n",
    ")\n",
    "if result.returncode == 0:\n",
    "    print('GPU:', result.stdout.strip())\n",
    "else:\n",
    "    print('No GPU detected — go to Notebook Settings > Accelerator and enable GPU.')\n",
    "    sys.exit('GPU required')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189fc787",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 2 — Clone OR-SIM repository\n",
    "\n",
    "> **`REPO_URL` is pre-filled with your GitHub repo.  No changes needed.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ef2dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "REPO_URL = 'https://github.com/Aditya-Lingam-9000/OR-SIM.git'\n",
    "REPO_DIR = '/kaggle/working/OR-SIM'\n",
    "\n",
    "if not os.path.isdir(os.path.join(REPO_DIR, '.git')):\n",
    "    subprocess.run(['git', 'clone', '--depth', '1', REPO_URL, REPO_DIR], check=True)\n",
    "    print(f'Cloned -> {REPO_DIR}')\n",
    "else:\n",
    "    subprocess.run(['git', '-C', REPO_DIR, 'pull', '--ff-only'], check=True)\n",
    "    print(f'Updated existing clone at {REPO_DIR}')\n",
    "\n",
    "# Add repo root to sys.path so backend.* packages are importable in all later cells\n",
    "if REPO_DIR not in sys.path:\n",
    "    sys.path.insert(0, REPO_DIR)\n",
    "\n",
    "# Show the 5 most recent commits as a sanity check\n",
    "subprocess.run(['git', '-C', REPO_DIR, 'log', '--oneline', '-5'], check=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf62ce36",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 3 — Install llama-cpp-python (CUDA 12.4 prebuilt wheel — no compilation)\n",
    "\n",
    "Kaggle T4 instances ship CUDA 12.4.  \n",
    "We use the prebuilt wheel from `abetlen.github.io` — identical to the Phase 3 approach.\n",
    "Takes ~30 s vs ~5 minutes if built from source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cd4dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.run(\n",
    "    [\n",
    "        sys.executable, '-m', 'pip', 'install',\n",
    "        'llama-cpp-python',\n",
    "        '--extra-index-url', 'https://abetlen.github.io/llama-cpp-python/whl/cu124',\n",
    "        '--quiet',\n",
    "    ],\n",
    "    check=True,\n",
    ")\n",
    "\n",
    "# Verify GPU offload was compiled in — MUST be True for decent latency\n",
    "import llama_cpp\n",
    "lib    = getattr(llama_cpp, 'llama_cpp', None)\n",
    "gpu_ok = lib.llama_supports_gpu_offload() if lib else False\n",
    "print(f'llama_supports_gpu_offload() = {gpu_ok}')\n",
    "if not gpu_ok:\n",
    "    raise RuntimeError(\n",
    "        'GPU offload not compiled in.  '\n",
    "        'Check nvidia-smi shows CUDA 12.x — the cu124 wheel requires CUDA >= 12.4.'\n",
    "    )\n",
    "\n",
    "from llama_cpp import Llama\n",
    "print('llama-cpp-python imported successfully')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6881396f",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 4 — Install OR-SIM Python dependencies\n",
    "\n",
    "There is no `requirements.txt` in the repo — we install each package explicitly.  \n",
    "`llama-cpp-python` was installed in Cell 3 from the CUDA wheel and is **intentionally omitted**\n",
    "here to prevent pip replacing it with the CPU-only build from PyPI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da9af56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PortAudio C library — required by sounddevice on headless Linux.\n",
    "# sounddevice is imported at module level inside backend/asr, so it must\n",
    "# be importable even though there is no real microphone on Kaggle.\n",
    "subprocess.run(\n",
    "    ['apt-get', 'install', '-y', '-q', 'libportaudio2'],\n",
    "    check=False,  # non-fatal — already present on some Kaggle images\n",
    ")\n",
    "\n",
    "PACKAGES = [\n",
    "    # Audio / ASR\n",
    "    'numpy',\n",
    "    'scipy',\n",
    "    'sounddevice',        # LiveTranscriber import (headless OK — device open only on mic use)\n",
    "    'onnxruntime',        # MedASR CTC model (CPU inference)\n",
    "    # Backend server\n",
    "    'loguru',             # structured logging\n",
    "    'pydantic>=2.0',      # Pydantic v2 data models\n",
    "    'fastapi',            # REST + WebSocket framework\n",
    "    'uvicorn[standard]',  # ASGI server (includes httptools + websocket extras)\n",
    "    'websockets',         # WebSocket transport layer\n",
    "    'httpx',              # async HTTP client\n",
    "    # Model download (used in Cell 5)\n",
    "    'huggingface_hub',\n",
    "]\n",
    "\n",
    "subprocess.run([sys.executable, '-m', 'pip', 'install', '--quiet'] + PACKAGES, check=True)\n",
    "print('All OR-SIM Python dependencies installed')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26ddae3",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 5 — Download MedGemma GGUF from HuggingFace\n",
    "\n",
    "Model: `unsloth/medgemma-4b-it-GGUF`  `medgemma-4b-it-Q3_K_M.gguf` (~2.1 GB)  \n",
    "Downloaded directly into the cloned repo at the path the backend expects.  \n",
    "**No dataset upload required** — identical to the Phase 3 approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb94a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "from pathlib import Path\n",
    "\n",
    "GGUF_DIR = Path(REPO_DIR) / 'models' / 'medgemma'\n",
    "GGUF_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "EXPECTED_GGUF = GGUF_DIR / 'medgemma-4b-it-Q3_K_M.gguf'\n",
    "\n",
    "# Skip download if already present (e.g. re-running the notebook)\n",
    "if EXPECTED_GGUF.exists():\n",
    "    MODEL_PATH = EXPECTED_GGUF\n",
    "    print(f'GGUF already present ({MODEL_PATH.stat().st_size / 1e9:.2f} GB) — skipping download.')\n",
    "else:\n",
    "    print('Downloading medgemma-4b-it-Q3_K_M.gguf from HuggingFace...')\n",
    "    print('(~2.1 GB — takes 2-5 minutes on Kaggle)')\n",
    "    MODEL_PATH = Path(hf_hub_download(\n",
    "        repo_id   = 'unsloth/medgemma-4b-it-GGUF',\n",
    "        filename  = 'medgemma-4b-it-Q3_K_M.gguf',\n",
    "        local_dir = str(GGUF_DIR),\n",
    "    ))\n",
    "\n",
    "print(f'Model path : {MODEL_PATH}')\n",
    "print(f'File size  : {MODEL_PATH.stat().st_size / 1e9:.2f} GB')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdf584a",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 6 — Verify MedASR ONNX model files\n",
    "\n",
    "`models/medasr/model.int8.onnx` and `tokens.txt` are committed to the repo.  \n",
    "This cell confirms they were cloned successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2891790",
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_dir = Path(REPO_DIR) / 'models' / 'medasr'\n",
    "all_ok  = True\n",
    "\n",
    "for fname in ('model.int8.onnx', 'tokens.txt'):\n",
    "    fpath = asr_dir / fname\n",
    "    if fpath.exists():\n",
    "        print(f'  {fname:<30}  ({fpath.stat().st_size / 1e6:.1f} MB)')\n",
    "    else:\n",
    "        print(f'MISSING: {fpath}')\n",
    "        all_ok = False\n",
    "\n",
    "if not all_ok:\n",
    "    raise FileNotFoundError(\n",
    "        'MedASR model files missing from cloned repo.\\n'\n",
    "        'Run locally: git add models/medasr/ && git push'\n",
    "    )\n",
    "print('MedASR model files verified')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d9fdd6",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 7 — Install pyngrok and configure your auth token\n",
    "\n",
    "1. Sign up (free) at https://ngrok.com  \n",
    "2. Copy your token from https://dashboard.ngrok.com/get-started/your-authtoken  \n",
    "3. Either paste it directly OR store it as a Kaggle Secret (recommended):\n",
    "   - *Notebook Settings  Add-ons  Secrets  Add* — key: `NGROK_TOKEN`\n",
    "   - Uncomment the `UserSecretsClient` lines and remove the placeholder string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecdb66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.run(\n",
    "    [sys.executable, '-m', 'pip', 'install', 'pyngrok', '--quiet'],\n",
    "    check=True,\n",
    ")\n",
    "from pyngrok import ngrok\n",
    "print('pyngrok installed')\n",
    "\n",
    "#  CONFIGURE \n",
    "# Option A — paste token directly (do NOT commit a notebook with a real token)\n",
    "NGROK_AUTH_TOKEN = 'YOUR_NGROK_AUTH_TOKEN'\n",
    "\n",
    "# Option B — Kaggle Secret (recommended for shared notebooks)\n",
    "# from kaggle_secrets import UserSecretsClient\n",
    "# NGROK_AUTH_TOKEN = UserSecretsClient().get_secret('NGROK_TOKEN')\n",
    "# \n",
    "\n",
    "if NGROK_AUTH_TOKEN == 'YOUR_NGROK_AUTH_TOKEN':\n",
    "    raise ValueError(\n",
    "        'Paste your real ngrok token above or use a Kaggle Secret.'\n",
    "        '  Get it from https://dashboard.ngrok.com/get-started/your-authtoken'\n",
    "    )\n",
    "\n",
    "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
    "print('ngrok auth token configured')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70170fd5",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 8 — Start the FastAPI backend server\n",
    "\n",
    "The server is launched as a **subprocess** (`python -m backend.server`).\n",
    "Using a subprocess (not a thread) avoids asyncio event-loop conflicts between uvicorn\n",
    "and Jupyter's own event loop — the most common cause of silent startup failures on Kaggle.\n",
    "\n",
    "MedGemma loads into GPU VRAM during startup — allow up to **60 seconds**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51a073d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket, time, urllib.request, json as _json\n",
    "\n",
    "# Stop any server left from a previous run of this cell\n",
    "try:\n",
    "    if _server_proc.poll() is None:\n",
    "        _server_proc.terminate()\n",
    "        _server_proc.wait(timeout=5)\n",
    "        print('Stopped previous server instance.')\n",
    "except NameError:\n",
    "    pass  # first run\n",
    "\n",
    "env = os.environ.copy()\n",
    "env['PYTHONPATH'] = REPO_DIR\n",
    "\n",
    "# python -m backend.server  is the canonical entry-point (same as local dev)\n",
    "_server_proc = subprocess.Popen(\n",
    "    [sys.executable, '-m', 'backend.server', '--host', '0.0.0.0', '--port', '8000'],\n",
    "    cwd=REPO_DIR,\n",
    "    env=env,\n",
    "    # stdout/stderr flow to the Kaggle cell output so startup logs are visible\n",
    ")\n",
    "print(f'Server PID: {_server_proc.pid}')\n",
    "print('Waiting for FastAPI to bind port 8000', end='', flush=True)\n",
    "\n",
    "deadline   = time.time() + 60\n",
    "connected  = False\n",
    "while time.time() < deadline:\n",
    "    if _server_proc.poll() is not None:\n",
    "        raise RuntimeError(\n",
    "            f'Server process exited early (exit code {_server_proc.returncode}).\\n'\n",
    "            'Scroll up for the traceback.'\n",
    "        )\n",
    "    try:\n",
    "        with socket.create_connection(('127.0.0.1', 8000), timeout=1):\n",
    "            connected = True\n",
    "            break\n",
    "    except OSError:\n",
    "        print('.', end='', flush=True)\n",
    "        time.sleep(1)\n",
    "\n",
    "print()\n",
    "if not connected:\n",
    "    raise TimeoutError('Server did not bind within 60 s — check the logs above.')\n",
    "\n",
    "# Quick health-check via plain urllib (no extra libraries needed)\n",
    "try:\n",
    "    with urllib.request.urlopen('http://127.0.0.1:8000/api/health', timeout=5) as r:\n",
    "        health = _json.loads(r.read())\n",
    "    print(f'Server healthy: {health}')\n",
    "except Exception as e:\n",
    "    print(f'Health check failed: {e}  — server may still be loading MedGemma, re-run this cell.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3ea3e6",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 9 — Open ngrok tunnel and print the public URLs\n",
    "\n",
    "**Copy the HTTPS URL printed below — you will paste it into `frontend/.env.local`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fe2547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close any stale tunnels from previous runs\n",
    "for _t in ngrok.get_tunnels():\n",
    "    ngrok.disconnect(_t.public_url)\n",
    "\n",
    "tunnel      = ngrok.connect(8000, bind_tls=True)\n",
    "NGROK_HTTPS = tunnel.public_url\n",
    "NGROK_WSS   = NGROK_HTTPS.replace('https://', 'wss://')\n",
    "\n",
    "print()\n",
    "print('=' * 65)\n",
    "print('  OR-SIM BACKEND IS LIVE')\n",
    "print('=' * 65)\n",
    "print(f'  HTTPS (REST API) : {NGROK_HTTPS}')\n",
    "print(f'  WSS  (WebSocket) : {NGROK_WSS}/ws/state')\n",
    "print('=' * 65)\n",
    "print()\n",
    "print('Copy the HTTPS URL, then follow Cell 10 on your local machine.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910cad93",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 10 — Connect the local frontend  *(run on YOUR machine, not Kaggle)*\n",
    "\n",
    "### Step 1 — Create `frontend/.env.local`\n",
    "\n",
    "```\n",
    "# d:\\OR-SIM\\frontend\\.env.local\n",
    "VITE_BACKEND_URL=https://abcd-1234.ngrok-free.app\n",
    "```\n",
    "\n",
    "Replace `https://abcd-1234.ngrok-free.app` with the **HTTPS URL from Cell 9**. No trailing slash.\n",
    "\n",
    "PowerShell one-liner (edit the URL first):\n",
    "```powershell\n",
    "'VITE_BACKEND_URL=https://abcd-1234.ngrok-free.app' | Out-File -Encoding utf8 d:\\OR-SIM\\frontend\\.env.local\n",
    "```\n",
    "\n",
    "### Step 2 — Start the Vite dev server\n",
    "\n",
    "```powershell\n",
    "cd d:\\OR-SIM\\frontend\n",
    "npm run dev\n",
    "```\n",
    "\n",
    "Expected output:\n",
    "```\n",
    "  VITE v6.x.x  ready in ~641 ms\n",
    "  -> Local:   http://localhost:5173/\n",
    "```\n",
    "\n",
    "### Step 3 — Open the simulator\n",
    "\n",
    "1. Open **http://localhost:5173** in Chrome or Edge.\n",
    "2. Pick a surgery (Heart Transplant / Liver Resection / Kidney PCNL).\n",
    "3. Click **Start Session**.\n",
    "4. Allow microphone access when the browser asks.\n",
    "5. Speak surgical commands — machines glow ON/OFF in the 3D room in real-time!\n",
    "\n",
    "### Stopping\n",
    "\n",
    "- Click **Stop Session** in the browser.\n",
    "- Stop the Kaggle kernel (square button) to free GPU VRAM.\n",
    "- The ngrok tunnel closes automatically with the kernel.\n",
    "\n",
    "### Troubleshooting\n",
    "\n",
    "| Symptom | Fix |\n",
    "|---|---|\n",
    "| WS status dot stays red | Check `.env.local` — no trailing slash, correct URL |\n",
    "| `ERR_NGROK_3200` | Free tier allows 1 tunnel; close other open ngrok sessions |\n",
    "| `ModuleNotFoundError: backend` | Re-run Cell 2 (sys.path), then Cell 8 |\n",
    "| `sounddevice` ImportError | Re-run Cell 4 (installs libportaudio2) |\n",
    "| Server exits immediately | Scroll up in Cell 8 for the Python traceback |\n",
    "| GGUF not found | Check Cell 5 printed a valid file size |\n",
    "| Kaggle kernel idle timeout | Re-run Cells 8 and 9 to restart server and tunnel |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c55cef",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 11 — Sanity check: confirm tunnel reaches the API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0837e299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ngrok adds a browser-warning page for unauthenticated requests.\n",
    "# The header below skips it so we get the raw JSON response.\n",
    "SKIP_WARN = {'ngrok-skip-browser-warning': '1'}\n",
    "\n",
    "for path, label in [('/api/health', 'Health'), ('/api/state', 'State')]:\n",
    "    url = f'{NGROK_HTTPS}{path}'\n",
    "    try:\n",
    "        req = urllib.request.Request(url, headers=SKIP_WARN)\n",
    "        with urllib.request.urlopen(req, timeout=10) as r:\n",
    "            data = _json.loads(r.read())\n",
    "        preview = _json.dumps(data, indent=2)[:400]\n",
    "        print(f'{label}:\\n{preview}\\n')\n",
    "    except Exception as e:\n",
    "        print(f'{label} check failed: {e}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6169a752",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 12 — Keep-alive ping *(optional — run while using the simulator)*\n",
    "\n",
    "Kaggle kills idle kernels after ~30 minutes.  Interrupt the cell (square button) to stop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8007636",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "PING_INTERVAL = 60     # seconds between pings\n",
    "MAX_PINGS     = 720    # 720 x 60 s = 12 hours (full Kaggle session limit)\n",
    "\n",
    "print(f'Keep-alive running — ping every {PING_INTERVAL}s.  Stop with the interrupt button.')\n",
    "for _i in range(1, MAX_PINGS + 1):\n",
    "    time.sleep(PING_INTERVAL)\n",
    "\n",
    "    if _server_proc.poll() is not None:\n",
    "        print(f'Server process died (code {_server_proc.returncode}).  Re-run Cell 8 + 9.')\n",
    "        break\n",
    "\n",
    "    try:\n",
    "        _req = urllib.request.Request(f'{NGROK_HTTPS}/api/health', headers=SKIP_WARN)\n",
    "        with urllib.request.urlopen(_req, timeout=5) as _r:\n",
    "            _status = _r.status\n",
    "    except Exception:\n",
    "        _status = 'error'\n",
    "\n",
    "    _ts = datetime.datetime.now(datetime.timezone.utc).strftime('%H:%M:%S UTC')\n",
    "    print(f'  [{_ts}] ping #{_i:03d} -> HTTP {_status}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
